---
title: "Frequentist and Bayesian Logistic Regression for Loan Repayment Analysis"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
options(digits = 3)
```

In this section, we explore the second component of the prompt, the
factors associated with loan repayment , via regression analysis. We
will be utilizing two different approaches in our analysis to
demonstrate the two principle philosophies of statistical analysis:
Frequentist and Bayesian analysis. While Frequentists assumes that the
parameters of interest are real fixed quantities whose values determine
the distribution of the data, Bayesians assume that only the data is
real and that the parameters of interest are themselves random
variables. In practice, the principle differences in these methods are
the incorporation of prior information into Bayesian analysis,
estimation techniques, and the method of uncertainty quantification.

In the tutorial below, we setup our $\texttt{R}$ environment, provide
some basic background on logistic regression, prepare the data, fit
Frequentist and Bayesian logistic regressions to address the prompt
above, and compare the results and performance of the two models.

# Setup

First, we load the necessary packages:

```{r}
library(tidyverse)
library(loo)
library(countrycode)
library(broom)
library(kableExtra)
library(pROC)
library(rstanarm)
library(bayesplot)
library(readr)
```

In addition to the familiar $\texttt{Tidyverse}$ packages, our analysis
will make use of several packages relevant to logistic regression and
Bayesian analysis. The $\texttt{loo}$ package allows us to compute
efficient approximate leave-one-out cross-validation for fitted Bayesian
models. The $\texttt{pROC}$ displays ROC curves which we will use to
evaluate our models. The $\texttt{rstanarm}$ is an appendage to the
$\texttt{rstan}$ package, the R interface to Stan which features an
expressive probabilistic programming language for specifying Bayesian
models backed by extensive math and algorithm libraries to support
automated computation. The $\texttt{bayesplot}$ package offers a variety
of plots of posterior draws, visual MCMC diagnostics, and graphical
posterior predictive checks. Additionally, the packages $\texttt{broom}$
and $\texttt{kableExtra}$ are used here to transform the messy model
output into clear tables.

```{r eval=FALSE, echo=FALSE}
percent <- training %>% 
  drop_na(sector) %>% # drop missing values by health variable
  group_by(sector) %>%  # specify categorical variable
  summarize(Frequency = n()) %>% # return counts / frequencies
  mutate(Percent = paste0(round(Frequency / nrow(training) * 100, 2), "%"))

table <- kbl(percent, 
    caption = "Table 2: Frequency table for loan sectors") %>%
    kable_styling(bootstrap_options = "striped", full_width = FALSE, position = "left")
table
colourCount = length(unique(loans$sector))
getPalette = colorRampPalette(brewer.pal(12, "Set3"))
bar <- ggplot(data = percent, aes(x = reorder(sector, Frequency), y = Frequency)) +
  geom_col(aes(fill = getPalette(colourCount)) , show.legend = FALSE) +
  ggtitle(paste("Loan Sector Distribution")) +
  coord_flip() 
bar
```

```{r eval=FALSE, echo=FALSE}
status <- loans %>% 
  drop_na(status) %>% # drop missing values by health variable
  group_by(status) %>%  # specify categorical variable
  summarize(Frequency = n()) %>% # return counts / frequencies
  mutate(Percent = paste0(round(Frequency / nrow(loans) * 100, 2), "%"))

table1 <- tibble(status) %>% 
  arrange(desc(Percent))
table1
donut1 <- ggplot(status, aes(x = 2, y = Frequency, fill = status)) +
    geom_bar(stat = "identity") +
    coord_polar(theta = "y", start = 0) + 
    theme_void() + # these theme removes the lines around chart and grey background
    theme(legend.title = element_text(size = 12),
          legend.text = element_text(size = 12)) +
    scale_fill_brewer(palette="Accent") +
    labs(fill = "Status Distribution") +
    xlim(0.5, 2.5)
donut1
bar1 <- ggplot(loans, aes(x = status)) +
  geom_bar() +
  coord_flip()
bar
```

```{r eval=FALSE, echo=FALSE}
ggplot(data = loan, aes(x = funded_amount)) +
  geom_histogram(binwidth=100) +
  labs(title="Distribution of Funded Amount")

ggplot(data = loan, aes(x = paid_amount)) +
  geom_histogram(binwidth=100) +
  labs(title="Distribution of Paid Amount")

ggplot(loan, aes(x = posted_yr)) +
  geom_histogram()
```

```{r eval=FALSE, echo=FALSE}
loan %>%
  count(status, sector) %>%       
  group_by(status) %>%
  mutate(pct= prop.table(n) * 100) %>%
  ggplot() + aes(status, pct, fill=sector) +
  geom_bar(stat="identity") +
  labs(x = "Status", y = "Proportion") +
  coord_flip() 
```

```{r eval=FALSE, echo=FALSE}
#continent
ggplot(data=d, aes(x = fct_infreq(continent),  fill=continent)) +
  geom_bar(stat = 'count') +
  scale_fill_brewer(palette = "Accent") +
  labs(x = "Continent")

ggplot(data=d, aes(x = status)) +
  geom_bar(aes(fill=as.factor(continent)), position="fill") +
  scale_fill_brewer(palette = "Accent")

#sector
ggplot(data=d, aes(x = status)) +
  geom_bar(aes(fill=as.factor(sector)), position="fill") 

#loan_amount
ggplot(d, aes(x = loan_amount, group = status)) +
  geom_boxplot()
  #geom_boxplot(outlier.colour="red", outlier.shape=8, outlier.size=4)
```

# Introduction to Logistic Regression

For this analysis, the response of interest is the loan repayment
status, recorded in the `status` column of the `loans` data set. It is a
categorical variable with 8 levels: "paid", "refunded", "defaulted",
"in_repayment", "expired", "inactive", "fundraising", and "funded".
Intuitively, for such a categorical response variable, one appropriate
model would be a multinomial logistic regression which allows for more
than two categories of the response variable. However, the
interpretation of results is not as straightforward in such a model -
and since the prompt is explicitly concerned with loan default, a more
interpretable model would be one which dichotomizes the data to contrast
defaulted and non-defaulted loans.

To achieve this, we restrict the dataset to completed loans, which are
either repaid or not repaid. With a binary response variable, we can
utilize a binary logistic regression. Note that simple linear regression
would not be appropriate here for several reasons: linear regression can
generate predictions larger than 1 or smaller than 0 which are not
sensible for classification, additionally, linear regression assumes
normal errors, which is not the case for a Bernoulli random variable.

The logistic regression model is an example of a broad class of models
known as generalized linear models (GLM's). GLM's extend ordinary least
squares regression to response variables from arbitrary distributions
through the use of a link function; instead of the response varying
linearly with the covariates, its link function does. In this case, our
response is a binary variable and so the Bernoulli model is appropriate.
The Bernoulli distribution is a discrete probability distribution of a
random variable which takes the value 1 with probability $p$ and the
value 0 with probability $q=1-p$. The likelihood for one observation $y$
can be written as a Bernoulli PMF over possible outcomes $k$:
$$\begin{equation}
    f(k;p) =
    \left\{
        \begin{array}{cc}
                p & \mathrm{if\ } k=1 \\
                1-p & \mathrm{if\ } k=0 \\
        \end{array} 
    \right.
\end{equation},$$

The two most common link functions used for binomial GLMs are the logit
and probit functions. For our analysis, we utilize the logit link due to
its more interpretable output. The logit link converts probabilities to
log odds, and therefore the model coefficients can be easily interpreted
in terms of odds ratios. When the logit link function is used the model
is often referred to as a logistic regression model (the inverse logit
function is the CDF of the standard logistic distribution).

The frequentist approach assumes data are random sample and parameters
are fixed while the Bayesian approach assumes both data and parameters
are random. The Bayesian analysis uses prior knowledge of the parameters
and the conditional distribution of the data given the parameters to be
employed to find the posterior distributions of the parameters given the
observed data. We would introduce the general procedures of these two
approaches In this section, we would apply two approaches of logistic
regression: frequentist and Bayesian, and compare the results.

# Data Preparation

For our analysis, we will be utilizing the `loans` file which provides
information at the loan-level. First, we read the dataset into our R
environment:

```{r}
loans <- read_csv("data/loans.csv")
```

## Response Variable

We begin by transforming the categorical `status` variable to be an
indicator for whether or not a loan was repaid, i.e. it takes on the
value one if the loan status is "paid" and zero if it is "defaulted."

```{r}
d <- loans %>% 
  dplyer::filter(status == "paid"|status == "defaulted")
d <- d %>% 
  mutate(status = if_else(status == "paid", 1, 0))
```

## Variable Selection

The response variable we were using is `status`. For the predictors, we
consider the following variables:

-   `loan_amount`, because we think that the smaller loans tend to be
    more easily paid back than the larger ones.

-   `sector`, sector for which loan is requested.

-   `posted_yr`, which is the year when the loan is posted on Kiva.

-   `continent`, a new variable created based on the country where the
    borrower is located, in order to reduce the number of levels of the
    geographical variable.

-   `time`, which is a variable we created representing the time from
    when the loan is posted to when the loan is funded, in months, since
    sometimes the time it takes for the borrowers to receive the loan
    may affect their financial situation and thus affect their ability
    to repay the loan according to the terms agreed.

-   `sex`, which indicates the sex of a borrower group, where "mixed"
    means there are both males and females in a borrower group.

```{r}
d <- d %>% 
dplyr::select(loan_id, loan_amount, status, funded_amount, posted_yr, posted_mo, posted_day, funded_yr, funded_mo, funded_day, sector, location.country, borrower_m_count, borrower_f_count) %>% 
mutate(continent = countrycode(sourcevar = location.country,
                            origin = "country.name",
                            destination = "continent"),
         time = (funded_yr - posted_yr)*365+(funded_mo - posted_mo)*30 + (funded_day - posted_day),
         sex = case_when(borrower_m_count == 0 ~ "female",
                         borrower_f_count == 0 ~ "male",
                         TRUE ~ "mixed"))
```

## Data Splitting

Next, we split our data into training set and test set, in order to
estimate the performance of our model on unseen data: data not used to
train. This is to see whether our model is generalized or overfitting.
Overfitting occurs when the model fits exactly against the training
data - it's memorizing the seen pattern rather than generalizing to new
data. We are sampling 80% of the data into the training set and the
other 20% into the testing set.

```{r}
seed <- 1
train <- sample(d$loan_id, nrow(d)*0.8)
training <- d %>% 
  filter(loan_id %in% train)
testing <-d %>% 
  filter(!loan_id %in% train)
```

# Frequentist Logistic Regression

We first fit the logistic regression model using a Frequentist approach.

## Model Fitting

We're fitting our model on the training set using the `glm()` function:

```{r}
model <- glm(status ~ loan_amount + sector + posted_yr + continent + time + sex, data = training, family = "binomial"(link = "logit")) 
```

Here we specify the family to be binomial and the link function to be
logit so that the function implements a logistic regression, described
above.

## Model Selection

To identify variables that are important in explaining variation in the
response, we perform model selection using the `step()` function.
Specifying "direction="both"" tells R to perform both forward and
backward selections. Forward selection here starts with an
intercept-only model and adds variables to it, until some stopping
criterion is met. Contrarily, backward selection starts with a full
model with all the variables included and then excludes variables from
that set until some stopping criterion is met. The default selection
criterion is $\mathrm{AIC}$: from the current model, it drops or adds
the one variable that leads to the best $\mathrm{AIC}$ improvement
(smallest $\mathrm{AIC}$).

```{r}
forback <- step(model, direction="both", trace=FALSE)
names(forback$model)
```

The selection results are stored in the `forback` list, and we can then
display only the names of selected variables instead of the whole output
by calling `names(forback$model)`. The variables selected are all except
`time`. We dropped the `time` variable from our model. The final model
is then:

```{r}
set.seed(0)
final <- glm(status ~ loan_amount + sector + posted_yr + continent + sex, data = training, family = "binomial"(link = "logit")) 
```

## Results

We can look at the summary of the model output using `tidy()` along with
functions in the $\texttt{kableExtra}$ package. The table is scrollable.

```{r}
tidy(final) %>% 
  kable(digits = 2) %>% 
  kable_styling(bootstrap_options = c("striped", "hover")) %>% 
  kable_paper() %>%
  scroll_box(width = "100%", height = "200px")
```

Before evaluating the results, we provide a brief review of the concept
of a p-value and a guide for interpreting the results of logistic
regression.

A p-value measures the probability of obtaining the observed results,
assuming that the null hypothesis - i.e., that the true effect is zero,
is true. The lower the p-value, the greater the statistical significance
of the observed difference. Thus, with p-values close to 1, these two
terms are not considered statistically significant. Hence, when browsing
our results, we will consider a coefficient to be noteworthy if it is
both statistically significant and has a point estimate with large
magnitude. For example, while the variable `continentOceania` has a
coefficient a considerably larger than others, it's p-value close to 1,
and so we cannot conclude that this is truly indicative of an
association rather than random chance.

Interpreting the coefficients in a logistic regression is a little less
straight-forward than in linear regression, but still quite simple. For
example, the coefficient for `posted_yr` is 0.45, which means that
increasing the loan posting time by one year is associated with an
increase in the odds of repayment by a factor of $e^{0.45}\approx1.65$:
loans occurring later in Kiva's history are more likely to be repaid,
which could indicate a change in the screening or application methods to
yield more reliable borrowers, or additional support to borrowers which
could have increased their ability to repay - this would be a good topic
to conduct some external research on.

In our sector analysis, Agriculture is set to be the baseline so all
sectors must be interpreted relative to that. Overall, the following
sectors are associated with large, significantly increased odds of
repayment relative to agriculture: Clothing, Construction, Food, Retail,
and Transportation. While Personal Use loans are associated with a
decrease in log odds of repayment, it is not statistically significant
in this model.

In our continent analysis, Africa is set to the baseline. We find that
all other continents are associated with greater log odds of repayment
than Africa and that these coefficients are highly significant - with
the exception of Oceania for which there are relatively few
observations. In particular, European and Asian borrowers are associated
with large increases in the log odds of repayment.

Finally, the model finds that male borrowers are associated with lower
log odds of repayment than female borrowers

### Model Evaluation

To evaluate our model, we plotted the ROC (*Receiver Operating
Characteristic*) curve. In a nutshell, ROC curve visualizes the
performance of a logistic model at all classification thresholds. It
plots *False positive rate* on the X-axis, which informs us about the
proportion of the negative class classified as positive
($\text{False positive rate} =\frac{\text{False positives}}{\text{False positives + True negatives (= All negatives)}}$)
and *True positive rate* on the y-axis, which informs us about the
proportion of the positive class correctly classified
($\text{True positive rate}=\frac{\text{True positives}}{\text{True positives + False negatives (= All positives)}}$).
We used `ggroc()` from the $\texttt{pROC}$ package to plot the ROC
curve.

```{r}
predicted1 <- predict(final, newdata = testing)
roc1 = roc(response = testing$status, predictor = predicted1)
ggroc(roc1, legacy.axes = TRUE) +
  labs(x = 'False-positive rate', y = 'True-positive rate', title = 'Simulated ROC curve')
```

As we can see from the graph, the curve is monotonic increasing. This is
because both False positive rate and True positive rate are increased
with lower the classification threshold which classifies more items as
positive. The closer the ROC curve is to the top left corner of the
plot, the better the model does at classifying the data into correct
categories. To quantify this, we can calculate the AUC (area under the
curve) which tells us how much of the plot is located under the curve.
AUC ranges in value from 0 to 1. A model whose predictions are 100%
wrong has an AUC of 0; one whose predictions are 100% correct has an AUC
of 1. The AUC score would be 1 in that scenario. We used the `auc()`
function from the $\texttt{pROC}$ package to calculate the AUC of the
model.

```{r}
auc(testing$status,predicted1)
```

This model yields an AUC score of 0.7008, which means there is about 70%
chance that our model will be able to distinguish between positive class
(paid) and negative class (defaulted). In general, an AUC of 0.5
suggests no discrimination, 0.7 to 0.8 is considered acceptable, 0.8 to
0.9 is considered excellent, and more than 0.9 is considered
outstanding.

### Discussion

Based on the Frequentist logistic regression, we find that while
controlling for sector, loan year, and borrower sex, Europe is
associated with the greatest increase in the odds of loan repayment
among all the contintents, followed by Asia and Americas. In terms of
sectors, we find that relative to Agriculture, all sectors except for
Personal Use are associated with an increase in the log odds of
repayment. The last noteworthy finding is that male borrowers are
associated with a large decline in the log odds of repayment.

## Bayesian Approach

Next, we implement an alternative to the model above using a Bayesian
approach. While a frequentist assumes that there are true values of the
parameters of the model and computes the point estimates of the
parameters, a Bayesian asserts that only data are real, and treats the
model parameters as random variables whose uncertainty can be
characterized by probability distributions.

### Model Fitting

To implement our Bayesian logistic model, we utilize an algorithm known
as Hamiltonian Monte Carlo. In particular, we use the program Stan, via
its $\texttt{R}$ interface in the $\texttt{rstanarm}$ package.

The `rstanarm` equivalent of `glm()` is `stan_glm()`, which supports
every link function that `glm()` supports. With `stan_glm()`, binomial
models with a logit link function can typically be fit slightly faster
than the identical model with a probit link.

In the following code, we fit a logistic model on the same training set
as in the Frequentist section, with specified link and the default
priors - scaled normal distributions - for the intercept and the
predictor coefficients. In contrast with the Frequentist procedure, in
Bayesian estimation we need to specify priors for our parameters, which
permit us to incorporate existing knowledge of the parameters into our
model. However, in this case we do not have any experts to consult with
and so we default to a weakly informative prior.

```{r eval=FALSE}
post <- stan_glm(status ~ loan_amount + sector + posted_yr + continent + sex, 
                 data = training,
                 family = binomial(link = "logit"), 
                 seed = seed,
                 refresh = 0,
                 cores=3, chains = 3,warmup = 500,iter = 1200)
```

Because model fitting using Stan can be time consuming, we save our
fitted models so that RStudio doesn't need to go through the model
fitting process every time we knit the document. Just load the models
from where they are stored.

```{r eval=FALSE, echo=FALSE}
save(post, file = "data/post.RData")
```

```{r include=FALSE}
load("data/post.Rdata")
load("data/post0.Rdata")
```

In order to get an idea of which variables are associated with changes
in the probability of repayment, we draw a caterpillar plot, which
displays the posterior medians and uncertainty intervals.

We sort the predictor variables in our model `post` by their posterior
medians. To do this, we extracted the `coefficient` element from `post`
and converted it into a data frame which had only one column
"post.coefficients". We then sort the rows based on the value of the
coefficients. Next, we extract the row names except for the intercept to
pass it as the `pars` parameter in the `mcmc_intervals()` function so
that it plots the 95% credible intervals (the dark blue bars) computed
from posterior draws with all chains merged in the order of ascending
median.

```{r}
# extract the coefficients and sort them in ascending order
coef <- data.frame(post$coefficients) %>% 
  arrange(post.coefficients)

# store the variables corresponding to the coefficients as a list
order <- rownames(coef)[-1]

# plot the sorted coefficients with uncertainty interval
cate <- mcmc_intervals(as.matrix(post), pars = order, prob = 0.95, prob_outer = 1)
```

### Results

A look at the caterpillar plot shows that `continentOceania` has
significantly larger median and uncertainty interval than other terms
and is thus compressing the plot to be difficult to read, so we remove
`continentOceania` from the caterpillar plot and replot it:

```{r}
coef <- coef %>% 
  filter(rownames(.)!="continentOceania")
p <- data.frame(post) %>% 
  select(-continentOceania)
order_new <- gsub(" ",".", rownames(coef)[-1])
cate_new <- mcmc_intervals(as.matrix(p), pars = order_new, prob = 0.95, prob_outer = 1)
cate_new
```

As in the Frequentist regression, Europe and Asia are associated with
increases in the log odds of repayment, while male borrowers, borrowers
in Agriculture and Personal Use sectors have decreased log odds of
repayment. We notice that the median estimate for `loan_amount` falls
exactly on the "0.0" line with no observable uncertainty interval,
indicating no relationship between loan amount and the loan status.

### Model Evaluation

```{r eval=FALSE}
(loo1 <- loo(post, save_psis = TRUE, k_threshold = 0.7))
save(loo0, file = "data/loo1.RData")
```

In the code chunk above, we assessed the strength of our model via its
posterior predictive LOOCV (Leave One Out Cross-Validation).
Cross-validation is a technique for evaluating models by training the
models on subsets of the available input data and evaluating them on the
complementary subset of the data. LOOCV is a type of cross-validation
approach in which each observation is considered as the validation set
and the rest ($N-1$) observations are considered as the training set.
PSIS-LOO CV is and efficient approximate LOOCV for Bayesian models using
Pareto smoothed importance sampling
([**PSIS**](https://www.rdocumentation.org/link/PSIS?package=loo&version=2.5.1&to=%3Dpsis)).
We can see from the output that PSIS-LOO result is reliable as all
Pareto k estimates are small (k\< 0.5). However as we know, this
accuracy rate is quite meaningless unless we have something to compare
it to. So we created a baseline model with no predictors for comparison:

```{r eval=FALSE}
post0 <- stan_glm(status ~ 1, data = training,
                 family = binomial(link = "logit"), 
                 seed = seed,
                 refresh = 0,
                 cores=3, chains = 3, warmup = 500,iter = 1200)
save(post0, file = "data/post0.RData")
```

```{r eval=FALSE}
(loo0 <- loo(post0, save_psis = T))
save(loo0, file = "data/loo0.RData")
```

```{r include=FALSE}
load("data/loo1.Rdata")
load("data/loo0.Rdata")
```

```{r}
rstanarm::compare_models(loo0, loo1)
```

Since difference is positive (38.6), the expected predictive accuracy
for the second model (post) is higher.

Below, we compute posterior predictive probabilities of the linear
predictor via the `posterior_linpred()` function provided in the
$\texttt{rstanarm}$ package. This function will extract posterior draws
from the linear predictor. If we used a link function, then specifying
the transform argument as `TRUE` will return the predictor as
transformed via the inverse-link.

```{r}
preds <- posterior_linpred(post, transform=TRUE)
pred <- colMeans(preds)
```

We calculate these posterior predictive probabilities in order to
determine the classification accuracy of our model. If the posterior
probability of paying back the loan for an borrower is greater or equal
to 0.5, then we would predict that observation to be a repaid loan (and
similarly for less than 0.5). For each observation, we can compare the
posterior prediction to the actual observed value. The proportion of
times we correctly predict an individual (i.e. prediction = 0 and
observation = 0 or prediction = 1 and observation = 1) is our
classification accuracy.

```{r}
pr <- as.integer(pred >= 0.5)
round(mean(xor(pr,as.integer(training$status==0))),3)
```

We should also evaluate the classification accuracy of our model on
unseen data. This can be done by using a test dataset or via a LOOCV
approach. Since we've talked about the former approach in the
frequentist section, here we would use the latter approach to illustrate
the function `E_loo()`, which uses importance weights generated from the
`loo()` function. Then we would compare the accuracy for the bayesian
and frequentist models on the same test data using the former approach.

```{r eval=FALSE}
ploo = E_loo(preds, loo1$psis_object, type = "mean", log_ratios = -log_lik(post))$value
save(ploo, file = "data/ploo.RData")
```

```{r include=FALSE}
load("data/ploo.RData")
```

```{r}
round(mean(xor(ploo > 0.5, as.integer(training$status == 0))), 3)
```

The accuracy is 0.977, which is very high. However, this does not
necessarily mean our model is doing great in prediction. Instead, it is
probably just due to the high unbalance of the data, with almost all
observations having the value "paid" for the response variable.

```{r}
qplot(pred, ploo,geom=c("point", "smooth")) +
  ggtitle("LOO probability vs. posterior predictive probability")
```

In the plot above we can see the small difference in posterior
predictive probabilities and LOO probabilities.

We can again use ROC and AUC to evaluate this model. First, we need to
prepare a test data set. Here we are extracting data from the original
data set that are not in the sample.

```{r}
predicted <- predict(post, newdata=testing)
```

Here's the ROC curve of our model on the testing data:

```{r}
roc = roc(response = testing$status, predictor = predicted)
ggroc(roc, legacy.axes = TRUE) +
  labs(x = 'False-positive rate', y = 'True-positive rate', title = 'Simulated ROC curve')
```

```{r}
auc(testing$status,predicted)
```

We got an AUC score of 0.7097, which is only slightly higher than the
one for the Frequentist model (0.7008).

## Conclusion

In both the Frequentist and Bayesian models, the probability of loans
getting repaid increases over time in the observed data, which is
generally a good sign for the loan market as well as the economy. Both
models suggests Oceania to have the highest odds ratio for the loan to
be repaid while Africa has the lowest odds ratio, followed by Europe,
Asia, America, and then Africa; they also have a high uncertainty on the
coefficients for Oceania due to low sample size. Loans in the Personal
Use and Agriculture sectors have the lowest log odds of repayment, while
Wholesale and Education are predicted to have the highest odds. Both
models provide strong evidence that female borrowers are more likely to
repay their loans. While the models have similar performance, when
evaluating on the testing set, the Bayesian model slightly outperforms
the Frequentist model, with an AUC score 0.009 higher.
