{
  "hash": "11f4835955b6e633d9d6f6d5033ef077",
  "result": {
    "markdown": "---\ntitle: \"Frequentist and Bayesian Logistic Regression for Loan Repayment Analysis\"\noutput:\n  html_document:\n    toc: true\n    toc_depth: 3\n    toc_float: true\neditor_options: \n  markdown: \n    wrap: 72\n---\n\n\n\n\nIn this section, we explore the second component of the prompt, the\nfactors associated with loan repayment , via regression analysis. We\nwill be utilizing two different approaches in our analysis to\ndemonstrate the two principle philosophies of statistical analysis:\nFrequentist and Bayesian analysis. While Frequentists assumes that the\nparameters of interest are real fixed quantities whose values determine\nthe distribution of the data, Bayesians assume that only the data is\nreal and that the parameters of interest are themselves random\nvariables. In practice, the principle differences in these methods are\nthe incorporation of prior information into Bayesian analysis,\nestimation techniques, and the method of uncertainty quantification.\n\nIn the tutorial below, we setup our $\\texttt{R}$ environment, provide\nsome basic background on logistic regression, prepare the data, fit\nFrequentist and Bayesian logistic regressions to address the prompt\nabove, and compare the results and performance of the two models.\n\n# Setup\n\nFirst, we load the necessary packages:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(loo)\nlibrary(countrycode)\nlibrary(broom)\nlibrary(kableExtra)\nlibrary(pROC)\nlibrary(rstanarm)\nlibrary(bayesplot)\nlibrary(readr)\n```\n:::\n\n\nIn addition to the familiar $\\texttt{Tidyverse}$ packages, our analysis\nwill make use of several packages relevant to logistic regression and\nBayesian analysis. The $\\texttt{loo}$ package allows us to compute\nefficient approximate leave-one-out cross-validation for fitted Bayesian\nmodels. The $\\texttt{pROC}$ displays ROC curves which we will use to\nevaluate our models. The $\\texttt{rstanarm}$ is an appendage to the\n$\\texttt{rstan}$ package, the R interface to Stan which features an\nexpressive probabilistic programming language for specifying Bayesian\nmodels backed by extensive math and algorithm libraries to support\nautomated computation. The $\\texttt{bayesplot}$ package offers a variety\nof plots of posterior draws, visual MCMC diagnostics, and graphical\nposterior predictive checks. Additionally, the packages $\\texttt{broom}$\nand $\\texttt{kableExtra}$ are used here to transform the messy model\noutput into clear tables.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n# Introduction to Logistic Regression\n\nFor this analysis, the response of interest is the loan repayment\nstatus, recorded in the `status` column of the `loans` data set. It is a\ncategorical variable with 8 levels: \"paid\", \"refunded\", \"defaulted\",\n\"in_repayment\", \"expired\", \"inactive\", \"fundraising\", and \"funded\".\nIntuitively, for such a categorical response variable, one appropriate\nmodel would be a multinomial logistic regression which allows for more\nthan two categories of the response variable. However, the\ninterpretation of results is not as straightforward in such a model -\nand since the prompt is explicitly concerned with loan default, a more\ninterpretable model would be one which dichotomizes the data to contrast\ndefaulted and non-defaulted loans.\n\nTo achieve this, we restrict the dataset to completed loans, which are\neither repaid or not repaid. With a binary response variable, we can\nutilize a binary logistic regression. Note that simple linear regression\nwould not be appropriate here for several reasons: linear regression can\ngenerate predictions larger than 1 or smaller than 0 which are not\nsensible for classification, additionally, linear regression assumes\nnormal errors, which is not the case for a Bernoulli random variable.\n\nThe logistic regression model is an example of a broad class of models\nknown as generalized linear models (GLM's). GLM's extend ordinary least\nsquares regression to response variables from arbitrary distributions\nthrough the use of a link function; instead of the response varying\nlinearly with the covariates, its link function does. In this case, our\nresponse is a binary variable and so the Bernoulli model is appropriate.\nThe Bernoulli distribution is a discrete probability distribution of a\nrandom variable which takes the value 1 with probability $p$ and the\nvalue 0 with probability $q=1-p$. The likelihood for one observation $y$\ncan be written as a Bernoulli PMF over possible outcomes $k$:\n\n$$\\begin{equation}\n    f(k;p) =\n    \\left\\{\n        \\begin{array}{cc}\n                p & \\mathrm{if\\ } k=1 \\\\\n                1-p & \\mathrm{if\\ } k=0 \\\\\n        \\end{array} \n    \\right.\n\\end{equation},$$\n\nThe two most common link functions used for binomial GLMs are the logit\nand probit functions. For our analysis, we utilize the logit link due to\nits more interpretable output. The logit link converts probabilities to\nlog odds, and therefore the model coefficients can be easily interpreted\nin terms of odds ratios. When the logit link function is used the model\nis often referred to as a logistic regression model (the inverse logit\nfunction is the CDF of the standard logistic distribution).\n\nThe frequentist approach assumes data are random sample and parameters\nare fixed while the Bayesian approach assumes both data and parameters\nare random. The Bayesian analysis uses prior knowledge of the parameters\nand the conditional distribution of the data given the parameters to be\nemployed to find the posterior distributions of the parameters given the\nobserved data. We would introduce the general procedures of these two\napproaches In this section, we would apply two approaches of logistic\nregression: frequentist and Bayesian, and compare the results.\n\n# Data Preparation\n\nFor our analysis, we will be utilizing the `loans` file which provides\ninformation at the loan-level. First, we read the dataset into our R\nenvironment:\n\n::: {.cell}\n\n```{.r .cell-code}\nloans <- read_csv(\"data/loans.csv\")\n```\n:::\n\n## Response Variable\n\nWe begin by transforming the categorical `status` variable to be an\nindicator for whether or not a loan was repaid, i.e. it takes on the\nvalue one if the loan status is \"paid\" and zero if it is \"defaulted.\"\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- loans %>% \n  filter(status == \"paid\"|status == \"defaulted\")\nd <- d %>% \n  mutate(status = if_else(status == \"paid\", 1, 0))\n```\n:::\n\n## Variable Selection\n\nThe response variable we were using is `status`. For the predictors, we\nconsider the following variables:\n\n-   `loan_amount`, because we think that the smaller loans tend to be\n    more easily paid back than the larger ones.\n\n-   `sector`, sector for which loan is requested.\n\n-   `posted_yr`, which is the year when the loan is posted on Kiva.\n\n-   `continent`, a new variable created based on the country where the\n    borrower is located, in order to reduce the number of levels of the\n    geographical variable.\n\n-   `time`, which is a variable we created representing the time from\n    when the loan is posted to when the loan is funded, in months, since\n    sometimes the time it takes for the borrowers to receive the loan\n    may affect their financial situation and thus affect their ability\n    to repay the loan according to the terms agreed.\n\n-   `sex`, which indicates the sex of a borrower group, where \"mixed\"\n    means there are both males and females in a borrower group.\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- d %>% \ndplyr::select(loan_id, loan_amount, status, funded_amount, posted_yr, posted_mo, posted_day, funded_yr, funded_mo, funded_day, sector, location.country, borrower_m_count, borrower_f_count) %>% \nmutate(continent = countrycode(sourcevar = location.country,\n                            origin = \"country.name\",\n                            destination = \"continent\"),\n         time = (funded_yr - posted_yr)*365+(funded_mo - posted_mo)*30 + (funded_day - posted_day),\n         sex = case_when(borrower_m_count == 0 ~ \"female\",\n                         borrower_f_count == 0 ~ \"male\",\n                         TRUE ~ \"mixed\"))\n```\n:::\n\n## Data Splitting\n\nNext, we split our data into training set and test set, in order to\nestimate the performance of our model on unseen data: data not used to\ntrain. This is to see whether our model is generalized or overfitting.\nOverfitting occurs when the model fits exactly against the training\ndata - it's memorizing the seen pattern rather than generalizing to new\ndata. We are sampling 80% of the data into the training set and the\nother 20% into the testing set.\n\n::: {.cell}\n\n```{.r .cell-code}\nseed <- 1\ntrain <- sample(d$loan_id, nrow(d)*0.8)\ntraining <- d %>% \n  filter(loan_id %in% train)\ntesting <-d %>% \n  filter(!loan_id %in% train)\n```\n:::\n\n# Frequentist Logistic Regression\n\nWe first fit the logistic regression model using a Frequentist approach.\n\n## Model Fitting\n\nWe're fitting our model on the training set using the `glm()` function:\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- glm(status ~ loan_amount + sector + posted_yr + continent + time + sex, data = training, family = \"binomial\"(link = \"logit\")) \n```\n:::\n\nHere we specify the family to be binomial and the link function to be\nlogit so that the function implements a logistic regression, described\nabove.\n\n## Model Selection\n\nTo identify variables that are important in explaining variation in the\nresponse, we perform model selection using the `step()` function.\nSpecifying \"direction=\"both\"\" tells R to perform both forward and\nbackward selections. Forward selection here starts with an\nintercept-only model and adds variables to it, until some stopping\ncriterion is met. Contrarily, backward selection starts with a full\nmodel with all the variables included and then excludes variables from\nthat set until some stopping criterion is met. The default selection\ncriterion is $\\mathrm{AIC}$: from the current model, it drops or adds\nthe one variable that leads to the best $\\mathrm{AIC}$ improvement\n(smallest $\\mathrm{AIC}$).\n\n::: {.cell}\n\n```{.r .cell-code}\nforback <- step(model, direction=\"both\", trace=FALSE)\nnames(forback$model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"status\"      \"loan_amount\" \"sector\"      \"posted_yr\"   \"continent\"  \n[6] \"sex\"        \n```\n:::\n:::\n\nThe selection results are stored in the `forback` list, and we can then\ndisplay only the names of selected variables instead of the whole output\nby calling `names(forback$model)`. The variables selected are all except\n`time`. We dropped the `time` variable from our model. The final model\nis then:\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(0)\nfinal <- glm(status ~ loan_amount + sector + posted_yr + continent + sex, data = training, family = \"binomial\"(link = \"logit\")) \n```\n:::\n\n## Results\n\nWe can look at the summary of the model output using `tidy()` along with\nfunctions in the $\\texttt{kableExtra}$ package. The table is scrollable.\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(final) %>% \n  kable(digits = 2) %>% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\")) %>% \n  kable_paper() %>%\n  scroll_box(width = \"100%\", height = \"200px\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div style=\"border: 1px solid #ddd; padding: 0px; overflow-y: scroll; height:200px; overflow-x: scroll; width:100%; \"><table class=\"table table-striped table-hover lightable-paper\" style='margin-left: auto; margin-right: auto; font-family: \"Arial Narrow\", arial, helvetica, sans-serif; margin-left: auto; margin-right: auto;'>\n <thead>\n  <tr>\n   <th style=\"text-align:left;position: sticky; top:0; background-color: #FFFFFF;\"> term </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> estimate </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> std.error </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> statistic </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> p.value </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:right;\"> -902.90 </td>\n   <td style=\"text-align:right;\"> 46.54 </td>\n   <td style=\"text-align:right;\"> -19.40 </td>\n   <td style=\"text-align:right;\"> 0.00 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> loan_amount </td>\n   <td style=\"text-align:right;\"> 0.00 </td>\n   <td style=\"text-align:right;\"> 0.00 </td>\n   <td style=\"text-align:right;\"> 2.51 </td>\n   <td style=\"text-align:right;\"> 0.01 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> sectorArts </td>\n   <td style=\"text-align:right;\"> 0.27 </td>\n   <td style=\"text-align:right;\"> 0.19 </td>\n   <td style=\"text-align:right;\"> 1.38 </td>\n   <td style=\"text-align:right;\"> 0.17 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> sectorClothing </td>\n   <td style=\"text-align:right;\"> 0.40 </td>\n   <td style=\"text-align:right;\"> 0.12 </td>\n   <td style=\"text-align:right;\"> 3.31 </td>\n   <td style=\"text-align:right;\"> 0.00 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> sectorConstruction </td>\n   <td style=\"text-align:right;\"> 0.67 </td>\n   <td style=\"text-align:right;\"> 0.20 </td>\n   <td style=\"text-align:right;\"> 3.28 </td>\n   <td style=\"text-align:right;\"> 0.00 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> sectorEducation </td>\n   <td style=\"text-align:right;\"> 1.92 </td>\n   <td style=\"text-align:right;\"> 1.01 </td>\n   <td style=\"text-align:right;\"> 1.90 </td>\n   <td style=\"text-align:right;\"> 0.06 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> sectorEntertainment </td>\n   <td style=\"text-align:right;\"> 0.71 </td>\n   <td style=\"text-align:right;\"> 1.01 </td>\n   <td style=\"text-align:right;\"> 0.70 </td>\n   <td style=\"text-align:right;\"> 0.49 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> sectorFood </td>\n   <td style=\"text-align:right;\"> 0.49 </td>\n   <td style=\"text-align:right;\"> 0.09 </td>\n   <td style=\"text-align:right;\"> 5.50 </td>\n   <td style=\"text-align:right;\"> 0.00 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> sectorHealth </td>\n   <td style=\"text-align:right;\"> 0.70 </td>\n   <td style=\"text-align:right;\"> 0.29 </td>\n   <td style=\"text-align:right;\"> 2.38 </td>\n   <td style=\"text-align:right;\"> 0.02 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> sectorHousing </td>\n   <td style=\"text-align:right;\"> 0.49 </td>\n   <td style=\"text-align:right;\"> 0.24 </td>\n   <td style=\"text-align:right;\"> 2.03 </td>\n   <td style=\"text-align:right;\"> 0.04 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> sectorManufacturing </td>\n   <td style=\"text-align:right;\"> 0.88 </td>\n   <td style=\"text-align:right;\"> 0.29 </td>\n   <td style=\"text-align:right;\"> 3.04 </td>\n   <td style=\"text-align:right;\"> 0.00 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> sectorPersonal Use </td>\n   <td style=\"text-align:right;\"> -0.23 </td>\n   <td style=\"text-align:right;\"> 0.37 </td>\n   <td style=\"text-align:right;\"> -0.63 </td>\n   <td style=\"text-align:right;\"> 0.53 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> sectorRetail </td>\n   <td style=\"text-align:right;\"> 0.59 </td>\n   <td style=\"text-align:right;\"> 0.09 </td>\n   <td style=\"text-align:right;\"> 6.42 </td>\n   <td style=\"text-align:right;\"> 0.00 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> sectorServices </td>\n   <td style=\"text-align:right;\"> 0.17 </td>\n   <td style=\"text-align:right;\"> 0.11 </td>\n   <td style=\"text-align:right;\"> 1.58 </td>\n   <td style=\"text-align:right;\"> 0.11 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> sectorTransportation </td>\n   <td style=\"text-align:right;\"> 0.95 </td>\n   <td style=\"text-align:right;\"> 0.24 </td>\n   <td style=\"text-align:right;\"> 3.94 </td>\n   <td style=\"text-align:right;\"> 0.00 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> sectorWholesale </td>\n   <td style=\"text-align:right;\"> 1.36 </td>\n   <td style=\"text-align:right;\"> 0.72 </td>\n   <td style=\"text-align:right;\"> 1.89 </td>\n   <td style=\"text-align:right;\"> 0.06 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> posted_yr </td>\n   <td style=\"text-align:right;\"> 0.45 </td>\n   <td style=\"text-align:right;\"> 0.02 </td>\n   <td style=\"text-align:right;\"> 19.45 </td>\n   <td style=\"text-align:right;\"> 0.00 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> continentAmericas </td>\n   <td style=\"text-align:right;\"> 0.74 </td>\n   <td style=\"text-align:right;\"> 0.07 </td>\n   <td style=\"text-align:right;\"> 11.07 </td>\n   <td style=\"text-align:right;\"> 0.00 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> continentAsia </td>\n   <td style=\"text-align:right;\"> 1.51 </td>\n   <td style=\"text-align:right;\"> 0.08 </td>\n   <td style=\"text-align:right;\"> 18.19 </td>\n   <td style=\"text-align:right;\"> 0.00 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> continentEurope </td>\n   <td style=\"text-align:right;\"> 3.46 </td>\n   <td style=\"text-align:right;\"> 0.71 </td>\n   <td style=\"text-align:right;\"> 4.87 </td>\n   <td style=\"text-align:right;\"> 0.00 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> continentOceania </td>\n   <td style=\"text-align:right;\"> 15.12 </td>\n   <td style=\"text-align:right;\"> 150.96 </td>\n   <td style=\"text-align:right;\"> 0.10 </td>\n   <td style=\"text-align:right;\"> 0.92 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> sexmale </td>\n   <td style=\"text-align:right;\"> -0.20 </td>\n   <td style=\"text-align:right;\"> 0.07 </td>\n   <td style=\"text-align:right;\"> -2.83 </td>\n   <td style=\"text-align:right;\"> 0.00 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> sexmixed </td>\n   <td style=\"text-align:right;\"> 0.11 </td>\n   <td style=\"text-align:right;\"> 0.13 </td>\n   <td style=\"text-align:right;\"> 0.80 </td>\n   <td style=\"text-align:right;\"> 0.42 </td>\n  </tr>\n</tbody>\n</table></div>\n\n`````\n:::\n:::\n\nBefore evaluating the results, we provide a brief review of the concept\nof a p-value and a guide for interpreting the results of logistic\nregression.\n\nA p-value measures the probability of obtaining the observed results,\nassuming that the null hypothesis - i.e., that the true effect is zero,\nis true. The lower the p-value, the greater the statistical significance\nof the observed difference. Thus, with p-values close to 1, these two\nterms are not considered statistically significant. Hence, when browsing\nour results, we will consider a coefficient to be noteworthy if it is\nboth statistically significant and has a point estimate with large\nmagnitude. For example, while the variable `continentOceania` has a\ncoefficient a considerably larger than others, it's p-value close to 1,\nand so we cannot conclude that this is truly indicative of an\nassociation rather than random chance.\n\nInterpreting the coefficients in a logistic regression is a little less\nstraight-forward than in linear regression, but still quite simple. For\nexample, the coefficient for `posted_yr` is 0.45, which means that\nincreasing the loan posting time by one year is associated with an\nincrease in the odds of repayment by a factor of $e^{0.45}\\approx1.65$:\nloans occurring later in Kiva's history are more likely to be repaid,\nwhich could indicate a change in the screening or application methods to\nyield more reliable borrowers, or additional support to borrowers which\ncould have increased their ability to repay - this would be a good topic\nto conduct some external research on.\n\nIn our sector analysis, Agriculture is set to be the baseline so all\nsectors must be interpreted relative to that. Overall, the following\nsectors are associated with large, significantly increased odds of\nrepayment relative to agriculture: Clothing, Construction, Food, Retail,\nand Transportation. While Personal Use loans are associated with a\ndecrease in log odds of repayment, it is not statistically significant\nin this model.\n\nIn our continent analysis, Africa is set to the baseline. We find that\nall other continents are associated with greater log odds of repayment\nthan Africa and that these coefficients are highly significant - with\nthe exception of Oceania for which there are relatively few\nobservations. In particular, European and Asian borrowers are associated\nwith large increases in the log odds of repayment.\n\nFinally, the model finds that male borrowers are associated with lower\nlog odds of repayment than female borrowers\n\n### Model Evaluation\n\nTo evaluate our model, we plotted the ROC (*Receiver Operating\nCharacteristic*) curve. In a nutshell, ROC curve visualizes the\nperformance of a logistic model at all classification thresholds. It\nplots *False positive rate* on the X-axis, which informs us about the\nproportion of the negative class classified as positive\n($\\text{False positive rate} =\\frac{\\text{False positives}}{\\text{False positives + True negatives (= All negatives)}}$)\nand *True positive rate* on the y-axis, which informs us about the\nproportion of the positive class correctly classified\n($\\text{True positive rate}=\\frac{\\text{True positives}}{\\text{True positives + False negatives (= All positives)}}$).\nWe used `ggroc()` from the $\\texttt{pROC}$ package to plot the ROC\ncurve.\n\n::: {.cell}\n\n```{.r .cell-code}\npredicted1 <- predict(final, newdata = testing)\nroc1 = roc(response = testing$status, predictor = predicted1)\nggroc(roc1, legacy.axes = TRUE) +\n  labs(x = 'False-positive rate', y = 'True-positive rate', title = 'Simulated ROC curve')\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\nAs we can see from the graph, the curve is monotonic increasing. This is\nbecause both False positive rate and True positive rate are increased\nwith lower the classification threshold which classifies more items as\npositive. The closer the ROC curve is to the top left corner of the\nplot, the better the model does at classifying the data into correct\ncategories. To quantify this, we can calculate the AUC (area under the\ncurve) which tells us how much of the plot is located under the curve.\nAUC ranges in value from 0 to 1. A model whose predictions are 100%\nwrong has an AUC of 0; one whose predictions are 100% correct has an AUC\nof 1. The AUC score would be 1 in that scenario. We used the `auc()`\nfunction from the $\\texttt{pROC}$ package to calculate the AUC of the\nmodel.\n\n::: {.cell}\n\n```{.r .cell-code}\nauc(testing$status,predicted1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nArea under the curve: 0.698\n```\n:::\n:::\n\nThis model yields an AUC score of 0.7008, which means there is about 70%\nchance that our model will be able to distinguish between positive class\n(paid) and negative class (defaulted). In general, an AUC of 0.5\nsuggests no discrimination, 0.7 to 0.8 is considered acceptable, 0.8 to\n0.9 is considered excellent, and more than 0.9 is considered\noutstanding.\n\n### Discussion\n\nBased on the Frequentist logistic regression, we find that while\ncontrolling for sector, loan year, and borrower sex, Europe is\nassociated with the greatest increase in the odds of loan repayment\namong all the contintents, followed by Asia and Americas. In terms of\nsectors, we find that relative to Agriculture, all sectors except for\nPersonal Use are associated with an increase in the log odds of\nrepayment. The last noteworthy finding is that male borrowers are\nassociated with a large decline in the log odds of repayment.\n\n## Bayesian Approach\n\nNext, we implement an alternative to the model above using a Bayesian\napproach. While a frequentist assumes that there are true values of the\nparameters of the model and computes the point estimates of the\nparameters, a Bayesian asserts that only data are real, and treats the\nmodel parameters as random variables whose uncertainty can be\ncharacterized by probability distributions.\n\n### Model Fitting\n\nTo implement our Bayesian logistic model, we utilize an algorithm known\nas Hamiltonian Monte Carlo. In particular, we use the program Stan, via\nits $\\texttt{R}$ interface in the $\\texttt{rstanarm}$ package.\n\nThe `rstanarm` equivalent of `glm()` is `stan_glm()`, which supports\nevery link function that `glm()` supports. With `stan_glm()`, binomial\nmodels with a logit link function can typically be fit slightly faster\nthan the identical model with a probit link.\n\nIn the following code, we fit a logistic model on the same training set\nas in the Frequentist section, with specified link and the default\npriors - scaled normal distributions - for the intercept and the\npredictor coefficients. In contrast with the Frequentist procedure, in\nBayesian estimation we need to specify priors for our parameters, which\npermit us to incorporate existing knowledge of the parameters into our\nmodel. However, in this case we do not have any experts to consult with\nand so we default to a weakly informative prior.\n\n::: {.cell}\n\n```{.r .cell-code}\npost <- stan_glm(status ~ loan_amount + sector + posted_yr + continent + sex, \n                 data = training,\n                 family = binomial(link = \"logit\"), \n                 seed = seed,\n                 refresh = 0,\n                 cores=3, chains = 3,warmup = 500,iter = 1200)\n```\n:::\n\nBecause model fitting using Stan can be time consuming, we save our\nfitted models so that RStudio doesn't need to go through the model\nfitting process every time we knit the document. Just load the models\nfrom where they are stored.\n\n::: {.cell}\n\n:::\n\n\n\nIn order to get an idea of which variables are associated with changes\nin the probability of repayment, we draw a caterpillar plot, which\ndisplays the posterior medians and uncertainty intervals.\n\nWe sort the predictor variables in our model `post` by their posterior\nmedians. To do this, we extracted the `coefficient` element from `post`\nand converted it into a data frame which had only one column\n\"post.coefficients\". We then sort the rows based on the value of the\ncoefficients. Next, we extract the row names except for the intercept to\npass it as the `pars` parameter in the `mcmc_intervals()` function so\nthat it plots the 95% credible intervals (the dark blue bars) computed\nfrom posterior draws with all chains merged in the order of ascending\nmedian.\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract the coefficients and sort them in ascending order\ncoef <- data.frame(post$coefficients) %>% \n  arrange(post.coefficients)\n\n# store the variables corresponding to the coefficients as a list\norder <- rownames(coef)[-1]\n\n# plot the sorted coefficients with uncertainty interval\ncate <- mcmc_intervals(as.matrix(post), pars = order, prob = 0.95, prob_outer = 1)\n```\n:::\n\n### Results\n\nA look at the caterpillar plot shows that `continentOceania` has\nsignificantly larger median and uncertainty interval than other terms\nand is thus compressing the plot to be difficult to read, so we remove\n`continentOceania` from the caterpillar plot and replot it:\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef <- coef %>% \n  filter(rownames(.)!=\"continentOceania\")\np <- data.frame(post) %>% \n  select(-continentOceania)\norder_new <- gsub(\" \",\".\", rownames(coef)[-1])\ncate_new <- mcmc_intervals(as.matrix(p), pars = order_new, prob = 0.95, prob_outer = 1)\ncate_new\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\nAs in the Frequentist regression, Europe and Asia are associated with\nincreases in the log odds of repayment, while male borrowers, borrowers\nin Agriculture and Personal Use sectors have decreased log odds of\nrepayment. We notice that the median estimate for `loan_amount` falls\nexactly on the \"0.0\" line with no observable uncertainty interval,\nindicating no relationship between loan amount and the loan status.\n\n### Model Evaluation\n\n::: {.cell}\n\n```{.r .cell-code}\n(loo1 <- loo(post, save_psis = TRUE, k_threshold = 0.7))\nsave(loo0, file = \"data/loo1.RData\")\n```\n:::\n\nIn the code chunk above, we assessed the strength of our model via its\nposterior predictive LOOCV (Leave One Out Cross-Validation).\nCross-validation is a technique for evaluating models by training the\nmodels on subsets of the available input data and evaluating them on the\ncomplementary subset of the data. LOOCV is a type of cross-validation\napproach in which each observation is considered as the validation set\nand the rest ($N-1$) observations are considered as the training set.\nPSIS-LOO CV is and efficient approximate LOOCV for Bayesian models using\nPareto smoothed importance sampling\n([**PSIS**](https://www.rdocumentation.org/link/PSIS?package=loo&version=2.5.1&to=%3Dpsis)).\nWe can see from the output that PSIS-LOO result is reliable as all\nPareto k estimates are small (k\\< 0.5). However as we know, this\naccuracy rate is quite meaningless unless we have something to compare\nit to. So we created a baseline model with no predictors for comparison:\n\n::: {.cell}\n\n```{.r .cell-code}\npost0 <- stan_glm(status ~ 1, data = training,\n                 family = binomial(link = \"logit\"), \n                 seed = seed,\n                 refresh = 0,\n                 cores=3, chains = 3, warmup = 500,iter = 1200)\nsave(post0, file = \"data/post0.RData\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n(loo0 <- loo(post0, save_psis = T))\nsave(loo0, file = \"data/loo0.RData\")\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrstanarm::compare_models(loo0, loo1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel formulas: \n :  NULL\n :  NULLelpd_diff        se \n    450.0      38.6 \n```\n:::\n:::\n\nSince difference is positive (38.6), the expected predictive accuracy\nfor the second model (post) is higher.\n\nBelow, we compute posterior predictive probabilities of the linear\npredictor via the `posterior_linpred()` function provided in the\n$\\texttt{rstanarm}$ package. This function will extract posterior draws\nfrom the linear predictor. If we used a link function, then specifying\nthe transform argument as `TRUE` will return the predictor as\ntransformed via the inverse-link.\n\n::: {.cell}\n\n```{.r .cell-code}\npreds <- posterior_linpred(post, transform=TRUE)\npred <- colMeans(preds)\n```\n:::\n\nWe calculate these posterior predictive probabilities in order to\ndetermine the classification accuracy of our model. If the posterior\nprobability of paying back the loan for an borrower is greater or equal\nto 0.5, then we would predict that observation to be a repaid loan (and\nsimilarly for less than 0.5). For each observation, we can compare the\nposterior prediction to the actual observed value. The proportion of\ntimes we correctly predict an individual (i.e. prediction = 0 and\nobservation = 0 or prediction = 1 and observation = 1) is our\nclassification accuracy.\n\n::: {.cell}\n\n```{.r .cell-code}\npr <- as.integer(pred >= 0.5)\nround(mean(xor(pr,as.integer(training$status==0))),3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.977\n```\n:::\n:::\n\nWe should also evaluate the classification accuracy of our model on\nunseen data. This can be done by using a test dataset or via a LOOCV\napproach. Since we've talked about the former approach in the\nfrequentist section, here we would use the latter approach to illustrate\nthe function `E_loo()`, which uses importance weights generated from the\n`loo()` function. Then we would compare the accuracy for the bayesian\nand frequentist models on the same test data using the former approach.\n\n::: {.cell}\n\n```{.r .cell-code}\nploo = E_loo(preds, loo1$psis_object, type = \"mean\", log_ratios = -log_lik(post))$value\nsave(ploo, file = \"data/ploo.RData\")\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nround(mean(xor(ploo > 0.5, as.integer(training$status == 0))), 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.977\n```\n:::\n:::\n\nThe accuracy is 0.977, which is very high. However, this does not\nnecessarily mean our model is doing great in prediction. Instead, it is\nprobably just due to the high unbalance of the data, with almost all\nobservations having the value \"paid\" for the response variable.\n\n::: {.cell}\n\n```{.r .cell-code}\nqplot(pred, ploo,geom=c(\"point\", \"smooth\")) +\n  ggtitle(\"LOO probability vs. posterior predictive probability\")\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-32-1.png){width=672}\n:::\n:::\n\nIn the plot above we can see the small difference in posterior\npredictive probabilities and LOO probabilities.\n\nWe can again use ROC and AUC to evaluate this model. First, we need to\nprepare a test data set. Here we are extracting data from the original\ndata set that are not in the sample.\n\n::: {.cell}\n\n```{.r .cell-code}\npredicted <- predict(post, newdata=testing)\n```\n:::\n\nHere's the ROC curve of our model on the testing data:\n\n::: {.cell}\n\n```{.r .cell-code}\nroc = roc(response = testing$status, predictor = predicted)\nggroc(roc, legacy.axes = TRUE) +\n  labs(x = 'False-positive rate', y = 'True-positive rate', title = 'Simulated ROC curve')\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-34-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nauc(testing$status,predicted)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nArea under the curve: 0.702\n```\n:::\n:::\n\nWe got an AUC score of 0.7097, which is only slightly higher than the\none for the Frequentist model (0.7008).\n\n## Conclusion\n\nIn both the Frequentist and Bayesian models, the probability of loans\ngetting repaid increases over time in the observed data, which is\ngenerally a good sign for the loan market as well as the economy. Both\nmodels suggests Oceania to have the highest odds ratio for the loan to\nbe repaid while Africa has the lowest odds ratio, followed by Europe,\nAsia, America, and then Africa; they also have a high uncertainty on the\ncoefficients for Oceania due to low sample size. Loans in the Personal\nUse and Agriculture sectors have the lowest log odds of repayment, while\nWholesale and Education are predicted to have the highest odds. Both\nmodels provide strong evidence that female borrowers are more likely to\nrepay their loans. While the models have similar performance, when\nevaluating on the testing set, the Bayesian model slightly outperforms\nthe Frequentist model, with an AUC score 0.009 higher.\n",
    "supporting": [
      "regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\r\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}