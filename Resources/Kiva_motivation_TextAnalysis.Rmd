---
title: "Analysing Text Data to understand Lender Motivation"
date: "01/07/2022"
output: 
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

------------------------------------------------------------------------

## Introduction

The goal of this section is to understand what motivates people to lend money to developing-nation entrepreneur through text analysis. Text analysis is an effective way to parse text and structure data to extract insights that are valuable and meaningful. Our motivation for performing text analysis arises from the structure of our data. Our data contains two columns that are free text fields that captures lenders' reasons for lending and borrowers' reasons for their intended use of the loan amount. Additionally, only focusing on borrower characteristics poses the issue of selection bias which may be present in the data. Selection bias refers to the differences in our sample that may not be a complete representation of the actual population. We do not have information regarding the population of borrowers at Kiva. It is possible that our data is not randomly selected and only reflects a certain aspect of the population. Through text analysis we can look at the data from a different perspective without the bias affecting our inference.

We will perform analysis on those text fields and assess what may have made a lender favor a certain borrower.

## Setup

### Importing required libraries

Before we dive into the analysis, we need to import all the required packages. We can use various packages to analyze text data. In this analysis, the packages we have used are:

```{r libraries, warning=FALSE, echo=TRUE, message=FALSE}
#text analysis packages
library("tm")
library("SnowballC")
library("tidyverse")
library("tidytext")
library("tidyr")
library("textstem")

#visualization packages
library("wordcloud")
library("ggplot2")
library("plotly")
library("colorblindr")
library("viridis")

library("rmarkdown")
library("readr")
library("kableExtra")
options(readr.show_col_types = FALSE)
```

### Reading the data

We use the `lender_loans.csv` file for our analysis. We import these files using the `read_csv` function. We take a look at the data to get an idea of what columns of our interest look like. The columns of interest to us are:

-   `loan_because` : Lenders can mention the reason or their motivation for lending
-   `use` : Borrowers specify what they will use the loan money for

*The data set presented below is a subset of columns from the actual data*

```{r input_data, echo=TRUE, warning=FALSE, results='asis', message = FALSE}

kiva_ll <- read_csv("main files/lender_loans.csv")
data_preview <- subset(kiva_ll, select=c("loan_id","loan_because","use","loan_amount"))

#Specifying rows for which we have data to showcase various reasons provided by lenders for lending
X <- (data_preview[c(133,165,177,235,298),]) 

knitr::kable(X, "pipe")
```

## Analysis

We analyse the text from both Lenders and Borrowers perspectives. Through the lenders text field we can identify what are the most common words used by lenders when citing reasons for lending. Through the borrower's text field, the most common words used can give us an insight into what the loans are most commonly used for. We could further try to relate this with loan amount or number of lenders and see whether the use of certain words could drive a lender to lend or whether the use of some words help a loan to get funded versus common words used in loans that do not get funded.

### Lenders

------------------------------------------------------------------------

#### Creating a corpus

To start with the text analysis, we start with cleaning the columns that contain our text data. We convert it to a corpus - which is a format used for text analysis. This also makes the text analysis using the `tm` package easier.

On a closer look at the data, we can observe that there are multiple special characters present which are not recognized by the typical `UTF-8` encoding of the file. Therefore we first encode the field using `latin1` encoding so that it can recognize these special characters.

We also observe that each line in our data frame does not represent a unique lender or their reason for lending. We need these values to be unique so that the frequency of a specific word is not misrepresented. If that specific lender has multiple observations in the dataset, their reason for lending will reappear multiple times as well. This will sway the frequency of the words used and will not give us an accurate picture of the most common words.

We subset our data frame to only contain the unique `lender_ids` and their corresponding reasons to lend captured in the column `loan_because`.

```{r corpus, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
# To recognize special characters
Encoding(kiva_ll$loan_because) <- "latin1" 


loan_bec <- subset(kiva_ll, select=c("lender_id","loan_because"))
loan_bec <- unique(loan_bec[ , c("lender_id", "loan_because")])

# Convert input data to corpus
corpus <- Corpus(VectorSource(as.vector(loan_bec$loan_because))) 
```

#### Data Transformation

Once we have created our corpus, we transform our data. This mainly includes the following steps:

1.  Removing all special characters and punctuation

2.  Converting all text to lower case

3.  Removing all numbers

4.  Removing extra white spaces

5.  Removing stop words. Stop words refer to common words used in text which do not add any contextual value. For example: 'a','the','as' etc.

By performing these steps, we are able to get rid of the 'noise' in the data. For example, by converting to lowercase it helps to keep the data consistent for analysis. By removing stop words it helps to focus mainly on key words present in the text which are relevant to us.

```{r cleaning, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
#Define a function to replace characters with space
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))

corpus <- tm_map(corpus, toSpace, "/")
corpus <- tm_map(corpus, toSpace, "@")
corpus <- tm_map(corpus, toSpace, "\\|")

#Convert all text to lower case
corpus <- tm_map(corpus, content_transformer(tolower))

# Remove  numbers
corpus <- tm_map(corpus, removeNumbers)

# Remove english common stopwords
corpus <- tm_map(corpus, removeWords, stopwords("english"))

# Remove punctuation
corpus <- tm_map(corpus, removePunctuation)

# Eliminate extra white spaces
corpus <- tm_map(corpus, stripWhitespace)
```

#### Lemmatization

Lemmatization is a method used in text analysis to get the root of the word. It is a form of normalizing the data using vocabulary and morphological analysis. For example, in our data we may have the words "helping" and "helped". With the help of lemmatization we will be able to transform these words to their base form i.e. "help". Another method to do this is Stemming, however with stemming we only get the base word whereas lemmatization helps to retain the context of the word as well.

```{r lemming, echo=TRUE, results='hide', message=FALSE, warning=FALSE }
#Lemmatization
corpus <-tm_map(corpus, lemmatize_strings)
```

#### Word Frequency Matrix

This is a simple way to compare all the terms or words in our data with their respective frequency. Here each row represents a word and each column represents the index number of the observations in our data frame. Each cell in the matrix are integer values that represent the frequency of the word in the respective rows. We then sum these columns, to get a total frequency of each word.

We are interested in the most common words used by lenders. For this analysis we only look at the raw count of each term, however one can assign weights or normalize each row based on the frequency of the word to perform other type of calculations. We sort the matrix in descending order to obtain these words. We also convert the matrix to a data frame so that it is easier to read and visualize.

```{r matrix, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
# Build a term-document matrix
TextDoc_dtm <- TermDocumentMatrix(corpus)

# Convert data type to matrix
dtm_m <- as.matrix(TextDoc_dtm)

# Sort by descending value of frequency
dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)

#Convert to dataframe
dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)

```

#### Visualizing the data

**Bar Chart of top 10 most frequent words**

Once we have our word frequency data frame we can visualize it. First we plot a bar chart to see the 10 most frequent words used by lenders along with their frequency.

```{r barchart, echo=TRUE, fig.align='center',warning=FALSE}
# Plot the most frequent words

ggplot(dtm_d[1:10,])+ aes(x = reorder(word, -freq), y=freq)+
       geom_bar(stat="identity", fill = "blue", alpha=.6, with = 0.5)+
  xlab("Word")+
  ylab("Frequency")+
  theme(axis.text.x=element_text(size=12))+
  ggtitle("Top 10 most frequent words [Lenders]")

```

From our bar plot we can see that the most common word used is *help*, with a frequency of 869. The word *help* has close to double the frequency as compared to the next most commonly used word *people* . We could conclude that most lenders choose to lend in order to help entrepreneurs from developing nations.

**Word Cloud**

A word cloud is visual representation of words in the form of a cluster where the size of the word represents its frequency. For our word cloud, we plot all the words in the dataset which have a minimum frequency of 5. The results are not too different from the bar chart but this is a different way of analyzing text data and we also get to see more words that were used by the lenders for their motivation for lending

```{r wordcloud, echo= TRUE, fig.align='center'}
set.seed(1234)
wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 5,
          max.words=100, random.order=FALSE, rot.per=0.40,scale=c(4,0.75),
          colors=brewer.pal(8, "Dark2"))
```

This concludes our analysis of the `loan_because` column. We looked at our data from the Lender's perspective and observed that most common word used is **help**, as is evident from the bar chart as well. However, from the word cloud we can also observe that most of the words used by lenders seem to be in the sphere of helping people, indicating that lenders sign up to Kiva provide support and be of service to entrepreneurs in developing countries.

### Borrowers

------------------------------------------------------------------------

Next, we look at what were the most common words used by borrowers that may have motivated lenders to lend to them. To narrow down our analysis, we further separate our loans into two classes - funded and not funded. We further analyze if there is a trend in the words used by the borrowers whose loans get funded versus borrowers whose loans did not.

Our data contains a `status` column which indicates the current status of the loan. First we look at the unique values in this column and the number of loans associated with each respectively. Further, we use these values to create an indicator variable for whether a loan has been funded or not.

```{r status, echo= TRUE, messgae = FALSE}

#Looking at unique values and respective loan count in status column
kiva_ll %>% 
  group_by(status) %>% 
  summarize(count = length(unique(loan_id)))
```

The status column contains 7 unique values. We look at the data dictionary and Kiva website to get a better idea of what each term means. Based on that we make the classification of these terms to our 2 classes funded and not-funded:

-   Funded: $\texttt{funded, in_repayment, paid}$

-   Not Funded: $\texttt{expired, fundraising}$

-   We do not include the loans associated with the status $\texttt{defaulted}$ and $\texttt{refunded}$ in our analysis since they do not clearly fall in either of the classes we have created \*

```{r funding indicator,echo= TRUE, message = FALSE}

#creating a vector to subset data for values corresponding to funded and not funded
funding_vec <- c("expired","funded","fundraising","in_repayment","paid")

#subset data to values status column that belong to funding_vec vector.
funding_stat <- subset(kiva_ll, kiva_ll$status %in% funding_vec)

#creating a vector for values which are associated with funded loans
funded_vec <- c("paid","in_repayment","funded")

#create indicator variable for Funded or not funded loans
funding_stat <- funding_stat %>% 
  mutate(funding_status = if_else(funding_stat$status %in% funded_vec, "Funded","Not Funded"))

#Create dataframe with unique borrower id, use for loan and corresponding funding status
loan_use <- unique(funding_stat[ , c("borrower_image_id", "use","funding_status")])
```

We filter our dataset to these values and create our `funding_status` indicator variable. Next, we separate the data into two data frames - Funded loans and Not funded loans. We then perform the same text analysis steps as we did in the lenders section on the `use` column.

```{r borrower cleaning, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
Encoding(loan_use$use) <- "latin1" #To recognize special characters

funded_loan <- subset(loan_use,loan_use$funding_status == "Funded")
notfunded_loan <- subset(loan_use,loan_use$funding_status == "Not Funded")
#Subset data to relevant columns
# loan_use <- subset(loan_country, select=c("lender_id","borrower_image_id","use"))
# loan_use <- unique(loan_use[ , c("borrower_image_id", "use")])

funded_corpus_use <- Corpus(VectorSource(as.vector(funded_loan$use)))# Convert input data to a corpus
notfunded_corpus_use <- Corpus(VectorSource(as.vector(notfunded_loan$use)))

funded_corpus_use <- tm_map(funded_corpus_use, toSpace, "/")
funded_corpus_use <- tm_map(funded_corpus_use, toSpace, "@")
funded_corpus_use <- tm_map(funded_corpus_use, toSpace, "\\|")

notfunded_corpus_use <- tm_map(notfunded_corpus_use, toSpace, "/")
notfunded_corpus_use <- tm_map(notfunded_corpus_use, toSpace, "@")
notfunded_corpus_use <- tm_map(notfunded_corpus_use, toSpace, "\\|")


#Convert all text to lower case
funded_corpus_use <- tm_map(funded_corpus_use, content_transformer(tolower))
notfunded_corpus_use <- tm_map(notfunded_corpus_use, content_transformer(tolower))

# Remove  numbers
funded_corpus_use <- tm_map(funded_corpus_use, removeNumbers)
notfunded_corpus_use <- tm_map(notfunded_corpus_use, removeNumbers)

# Remove english common stopwords
funded_corpus_use <- tm_map(funded_corpus_use, removeWords, stopwords("english"))
notfunded_corpus_use <- tm_map(notfunded_corpus_use, removeWords, stopwords("english"))

# Remove punctuation
funded_corpus_use <- tm_map(funded_corpus_use, removePunctuation)
notfunded_corpus_use <- tm_map(notfunded_corpus_use, removePunctuation)

# Eliminate extra white spaces
funded_corpus_use <- tm_map(funded_corpus_use, stripWhitespace)
notfunded_corpus_use <- tm_map(notfunded_corpus_use, stripWhitespace)
```

```{r use matrix, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
# Build a term-document matrix
funded_TextDoc_dtm <- TermDocumentMatrix(funded_corpus_use)
notfunded_TextDoc_dtm <- TermDocumentMatrix(notfunded_corpus_use)

# Convert data type to matrix
funded_dtm_m <- as.matrix(funded_TextDoc_dtm)
notfunded_dtm_m <- as.matrix(notfunded_TextDoc_dtm)

# Sort by descending value of frequency
funded_dtm_v <- sort(rowSums(funded_dtm_m),decreasing=TRUE)
notfunded_dtm_v <- sort(rowSums(notfunded_dtm_m),decreasing=TRUE)

#Convert to dataframe
funded_dtm_d <- data.frame(word = names(funded_dtm_v),freq=funded_dtm_v)
notfunded_dtm_d <- data.frame(word = names(notfunded_dtm_v),freq=notfunded_dtm_v)

```

**Bar Chart**

First we plot two bar charts to visualize and compare the top 10 most frequently used words by borrowers whose loan have been funded and not funded.

```{r borrower barchart, echo=FALSE, out.width="50%", warning=FALSE, fig.show='hold'}
# Plot the most frequent words

par(mfrow=c(1,1)) 

#Borrower Bar Chart of top 10 frequent words
ggplot(funded_dtm_d[1:10,])+ aes(x = reorder(word, -freq), y=freq)+
       geom_bar(stat="identity", fill = "blue", alpha=.6,with = 0.5)+
  xlab("Word")+
  ylab("Frequency")+
  ggtitle("Top 10 most frequent words [Funded]")

ggplot(notfunded_dtm_d[1:10,])+ aes(x = reorder(word, -freq), y=freq)+
       geom_bar(stat="identity", fill = "blue", alpha=.6,with = 0.5)+
  xlab("Word")+
  ylab("Frequency")+
  ggtitle("Top 10 most frequent words [Not Funded]")

```

**Word Cloud**

Similar to the bar charts we create two word clouds, one representing the most common words used by the borrowers whose Loans have been funded and the other representing loans that did not get funded. We plot all the words in the dataset which have a minimum frequency of 5. The results are not too different from the bar chart but this is a different way of analyzing text data and we also get to see more words that were used by the lenders for their motivation for lending

```{r, echo=TRUE, header=FALSE, message = FALSE, }
#Table to see no. of funded and not funded loans

funding_loans_count <- funding_stat %>% group_by(funding_status) %>% summarize(loan_count = length(unique(loan_id)))

funding_loans_count %>% kbl(caption = "Value Counts of funding status") %>% 
  kable_classic(bootstrap_options ="striped",full_width = F, html_font = "helvetica",position = "left")

```

```{r, echo=TRUE,header= FALSE,out.width="50%", message = FALSE, warning = FALSE, fig.show='hold'}
#Borrower Wordcloud
set.seed(1234)

par(mfrow=c(1,1)) 

wordcloud(words = funded_dtm_d$word, freq = funded_dtm_d$freq, min.freq = 5,
          max.words=100, main ="Wordcloud [Borrowers]",random.order=FALSE, rot.per=0.40,scale=c(4,0.75),
          colors=brewer.pal(8, "Dark2"))

wordcloud(words = notfunded_dtm_d$word, freq = notfunded_dtm_d$freq, min.freq = 5,
          max.words=100, main ="Wordcloud [Borrowers]",random.order=FALSE, rot.per=0.40,scale=c(4,0.75),
          colors=brewer.pal(8, "Dark2"))

```

From the bar chart we can see that the word with highest frequency is **buy** followed by **purchase**. This implies that most borrowers are applying for loans to be able to source the raw materials for their businesses. The remaining 8 words have less than half frequency than of the top 2 words. It is interesting to note that both funded and not funded loans have the same 2 words with the highest frequency.

Based on the word cloud we also observe that there is not a lot of difference in the kinds of words used by borrowers who fall into the Funded class as opposed to those in the Not funded class. However, we must also factor in that the number of loans that belong to Funded class is much higher than the loans belonging to the Not Funded class (This can be seen from the table).

## Conclusion

To conclude, we looked at free text fields from both borrowers and lenders perspective to understand motivations for lending. We also looked at a breakdown of Funded and Not funded loans for borrowers. We observed that Lender's are usually motivated by altruism and most commonly lend money to help developing nation entrepreneurs. When it comes to borrowers, we observe that most borrowers opt for loans in order to source raw materials or to purchase resources for their business or workplace. Our analysis to conclude whether there were specific words used by borrowers that may have helped them get funded was not entirely conclusive. The words used by each type of borrower are very similar which could be indicative of the fact that there are other factors at play when it comes to getting a loan funded.