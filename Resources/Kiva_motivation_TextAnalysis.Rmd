---
title: "Analysing Text Data to understand Lender Motivation"
date: "01/07/2022"
output: 
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

------------------------------------------------------------------------

## Introduction

DEFINE TEXT ANALYSIS SOMEWHERE IN THIS PARAGRAPH

In this section we continue our analysis of part one of the prompt: motivations for lending. Whereas in the previous section we attempted to infer lender motivation from systematic differences in funded and non-funded borrowers, here we directly analyze the free responses provided by lenders using text analysis. Text analysis is an effective way to parse text and structure data to extract insights that are valuable and meaningful. The data contains two relevant columns containing free text quoting the self-reported lenders' reasons for lending and borrowers' reasons for their intended use of the loan amount. We will perform analysis on those text fields and assess what may have made a lender favor a certain borrower.

EXPLAIN HOW BORROWER MOTIVATION IS RELEVANT TO THE PROMPT - DOESN'T IT ASK ABOUT REASONS FOR LENDING? UNLESS YOU'RE COMPARING NON-FUNDED TO FUNDED BORROWERS I DON'T SEE HOW BORROWER MOTIVATION RELATES TO THIS; IF YOU'RE DOING THAT, MAKE IT EXPLICIT HERE.

## Setup

### Importing required libraries

Before we dive into the analysis, we need to import all the required packages. We can use various packages to analyze text data. In this analysis, the packages we have used are:

"WE CAN USE VARIOUS" IS NOT VERY INFORMATIVE - WHY DID YOU CHOOSE THE ONES YOU DID? WHAT KINDS OF TEXT ANALYSIS DO THEY PERFORM? WHAT ARE THE DIFFERENT KINDS OF TEXT ANALYSIS IN GENERAL?

```{r libraries, warning=FALSE, echo=TRUE, message=FALSE}
#text analysis packages
library("tm")
library("SnowballC")
library("tidyverse")
library("tidytext")
library("tidyr")
library("textstem")

#visualization packages
library("wordcloud")
library("ggplot2")
library("plotly")
library("colorblindr")
library("viridis")

library("rmarkdown")
library("readr")
library("kableExtra")
options(readr.show_col_types = FALSE)
```

### Reading the data

We use the `lender_loans.csv` file for our analysis. The columns of interest to us are:

-   `loan_because` : free response indicating lender's reason for lending
-   `use` : free response indicating a borrower's intended use (more flexible than "sector")

I TRIED TO MAKE THESE SOUND A LITTLE MORE PRECISE, BUT MAKE SURE THEY'RE CORRECT

WHAT PROPORTION OF VALUES ARE MISSING? ARE YOU MAKING ANY ASSUMPTION ABOUT MISSINGNESS IN YOUR ANALYSIS? MISSING AT RANDOM? IMPORTANT TO MENTION THAT HERE

*The data set presented below is a subset of columns from the actual data*

```{r input_data, echo=TRUE, warning=FALSE, results='asis', message = FALSE}

kiva_ll <- read_csv("main files/lender_loans.csv")
data_preview <- subset(kiva_ll, select=c("loan_id","loan_because","use","loan_amount"))

#Specifying rows for which we have data to showcase various reasons provided by lenders for lending
X <- (data_preview[c(133,165,177,235,298),]) 

knitr::kable(X, "pipe")
```

## Analysis

We analyse the text from both Lenders and Borrowers perspectives. From the lender text field we identify the most common words used by lenders when citing reasons for lending; from the borrower text field, the most common words used can give us an insight into what the loans are typically used for. We could further try to relate this with loan amount or number of lenders and see whether the use of certain words could drive a lender to lend or whether the use of some words help a loan to get funded versus common words used in loans that do not get funded.

COULD YOU OR DO YOU? MAKE IT MORE CLEAR FROM THIS WHAT ANALYSIS YOU ACTUALLY DO

TREAT THIS AS AN INTRODUCTION AND PROVIDE A SUMMARY OF WHAT YOU WILL DISCUSS IN THIS SECTION: CORPUS CREATION, DATA CLEANING, LEMMATIZATION, VISUALIZATION, ETC.

### Lenders

------------------------------------------------------------------------

#### Creating a corpus

Now that we've identified the relevant data, the first step towards text analysis is to clean the data and convert it to a corpus - a format used for text analysis, and the format most convenient for the $\texttt{tm}$ package which will be using.

Upon first inspection, we observe multiple special characters present which are not recognized by the typical `UTF-8` encoding of the file. Therefore we first encode the field using `latin1` encoding so that it can recognize these special characters.

We also observe that each line in our data frame does not represent a unique lender or their reason for lending. We need these values to be unique so that the frequency of a specific word is not misrepresented. If that specific lender has multiple observations in the dataset, their reason for lending will reappear multiple times as well. This will sway the frequency of the words used and will not give us an accurate picture of the most common words.

DEPENDS HOW YOU DEFINE ACCURATE - THIS WOULD GIVE YOU THE MOST COMMON REASON ON A LOAN-WEIGHTED BASIS, WHICH SEEMS NOT UNREASONABLE. IT'S OKAY IF YOU PREFER TO JUST COUNT EACH LENDER ONCE, BUT IT'S NOT CLEAR THAT ONE WAY IS MORE CORRECT THAN THE OTHER.

We subset our data frame to only contain the unique `lender_ids` and their corresponding reasons fir lending as captured in the column `loan_because`.

```{r corpus, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
# To recognize special characters
Encoding(kiva_ll$loan_because) <- "latin1" 


loan_bec <- subset(kiva_ll, select=c("lender_id","loan_because"))
loan_bec <- unique(loan_bec[ , c("lender_id", "loan_because")])

# Convert input data to corpus
corpus <- Corpus(VectorSource(as.vector(loan_bec$loan_because))) 
```

#### Data Transformation

Once we have created our corpus, we transform our data via the following steps:

1.  Removing all special characters and punctuation

2.  Converting all text to lower case

3.  Removing all numbers

4.  Removing extra white spaces

5.  Removing "stop words." Stop words refer to common words used in text which do not add any contextual value. For example: 'a','the','as', etc.

By performing these steps, we are able to get rid of the 'noise' in the data. For example, by converting to lowercase it helps to keep the data consistent for analysis. By removing stop words it helps to focus mainly on key words present in the text which are relevant to us.

```{r cleaning, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
#Define a function to replace characters with space
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))

corpus <- tm_map(corpus, toSpace, "/")
corpus <- tm_map(corpus, toSpace, "@")
corpus <- tm_map(corpus, toSpace, "\\|")

#Convert all text to lower case
corpus <- tm_map(corpus, content_transformer(tolower))

# Remove  numbers
corpus <- tm_map(corpus, removeNumbers)

# Remove english common stopwords
corpus <- tm_map(corpus, removeWords, stopwords("english"))

# Remove punctuation
corpus <- tm_map(corpus, removePunctuation)

# Eliminate extra white spaces
corpus <- tm_map(corpus, stripWhitespace)
```

#### Lemmatization

Lemmatization is a method used in text analysis to extract the root of each word in the corpus. It is a form of normalizing the data using vocabulary and morphological analysis. For example, in our data we may have the words "helping" and "helped". Lemmatization transforms both words to their root word, "help". Another method to do this is Stemming, however with stemming we only get the base word whereas lemmatization helps to retain the context of the word as well.

DIFFERENCE BETWEEN LEMMATIZAITON AND STEMMING IS UNCLEAR FROM YOUR DESCRIPTION. HOW DOES LEMMATIZATION RETAIN CONTEXT?

YOU SHOULD PROBABLY INTRODUCE THE WORD DICTIONARY AT SOME POINT, POSSIBLY IN A BRIEF INTRO TO TEXT ANALYSIS.

```{r lemming, echo=TRUE, results='hide', message=FALSE, warning=FALSE }
#Lemmatization
corpus <-tm_map(corpus, lemmatize_strings)
```

#### Word Frequency Matrix

Now that the data is prepared, we begin our analysis with a word frequency matrix, which displays the frequency of occurrence for each word in our dictionary. Each row represents a word and each column represents the number of times that word occurs in our data frame. Each cell in the matrix are integer values that represent the frequency of the word in the respective rows. We then sum these columns, to get a total frequency of each word.

THIS DESCRIPTION SEEMS UNNECESSARY TO ME AND POSSIBLY EVEN CONFUSES THINGS. HEAD A PORTION OF THE MATRIX HERE IF YOU THINK IT'S WORTH EXPLAINING, OTHERWISE JUST KEEP THE FIRST SENTENCE.

For this analysis we consider only the raw count of each term, however alternative weighting and normalization schemes are possible in the package as well. We convert the matrix to a data frame for ease of manipulation and sort it in terms of decreasing frequency.

```{r matrix, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
# Build a term-document matrix
TextDoc_dtm <- TermDocumentMatrix(corpus)

# Convert data type to matrix
dtm_m <- as.matrix(TextDoc_dtm)

# Sort by descending value of frequency
dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)

#Convert to dataframe
dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)

```

#### Visualizing the data

**Bar Chart of top 10 most frequent words**

Once we have our word frequency data frame we can visualize it. First we plot a bar chart to see the 10 most frequent words used by lenders along with their frequency. From our bar chart we see that the most commonly used word is *help*, with a frequency of 869, which is nearly twice that of the next most commonly used word, *people* . This suggests that an altruistic desire to help developing nation entrepreneurs may be a significant motivation for Kiva lenders.

ARE ALL BORROWERS ACTUALLY FROM DEVELOPING NATIONS?

```{r barchart, echo=TRUE, fig.align='center',warning=FALSE}
# Plot the most frequent words

ggplot(dtm_d[1:10,])+ aes(x = reorder(word, -freq), y=freq)+
       geom_bar(stat="identity", fill = "blue", alpha=.6, with = 0.5)+
  xlab("Word")+
  ylab("Frequency")+
  theme(axis.text.x=element_text(size=12))+
  ggtitle("Top 10 most frequent words [Lenders]")

```

**Word Cloud**

A word cloud is visual representation of words in the form of a cluster where the size of the word represents its frequency. For our word cloud, we plot all the words in the data set which have a minimum frequency of 5. While the interpretation is similar to the that of the bar chart, this alternative visualization allows us to see more words than could reasonably fit in a bar chart.

```{r wordcloud, echo= TRUE, fig.align='center'}
set.seed(1234)
wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 5,
          max.words=100, random.order=FALSE, rot.per=0.40,scale=c(4,0.75),
          colors=brewer.pal(8, "Dark2"))
```

This concludes our analysis of the `loan_because` column. We looked at our data from the Lender's perspective and observed that most common word used is **help**, as is evident from the bar chart as well. However, from the word cloud we can also observe that most of the words used by lenders seem to be in the sphere of helping people, indicating that lenders sign up to Kiva provide support and be of service to entrepreneurs in developing countries.

GET MORE SPECIFIC, THIS SHOULD BE LONGER. TRY TO INTERPRET A LITTLE MORE DEEPLY, THIS IS BASICALLY THE CULMINATION OF ALL OF YOUR TEXT ANALYSIS

### Borrower text analysis

------------------------------------------------------------------------

Next, we look for systematic differences in the words used by funded and non-funded borrowers when describing their intended use for the loans. This may suggest lender preferences and motivations. In this portion of the analysis, we rely on the `status` column, which indicates the current status of the loan, as well as the `use` column, which indicates the borrower's intended use for the requested loan amount.

To get a better idea of how many loans are funded, we look at the unique values in the `status` column and the number of loans associated with each value. Further, we use these values to create an indicator variable for whether a loan has been funded or not.

```{r status, echo= TRUE, messgae = FALSE}

#Looking at unique values and respective loan count in status column
kiva_ll %>% 
  group_by(status) %>% 
  summarize(count = length(unique(loan_id)))
```

FORMAT THIS AS A NICE TABLE LIKE YOU DO BELOW. COMMENT ON THE DISTRIBUTION OF FUNDED VERSUS NONFUNDED, ETC.

The status column contains 7 unique values. We look at the data dictionary and Kiva website to get a better idea of what each term means. Based on that we make the classification of these terms to our 2 classes funded and not-funded:

-   Funded: $\texttt{funded, in_repayment, paid}$

-   Not Funded: $\texttt{expired, fundraising}$

-   We do not include the loans associated with the status $\texttt{defaulted}$ and $\texttt{refunded}$ in our analysis since they do not clearly fall in either of the classes we have created \*

HOW DID YOU DECIDE TO CLASSIFY THESE - JUST WRITE A SENTENCE FOR EACH FUNDED AND NON-FUNDED. I WOULD THINK DEFAULTED AND REFUNDED SHOULD ACTUALLY GO IN THE FUNDED CATEGORY - TO DEFAULT OR REFUND YOU HAVE TO GET A LOAN IN THE FIRST PLACE. AND I'LL WE'RE INTERESTED IN IS THE MOMENT WHEN LENDERS DECIDE TO FUND ONE BORROWER RATHER THAN ANOTHER - WHAT HAPPENS TO THE LOAN AFTER THAT IS IRRELEVANT.

```{r funding indicator,echo= TRUE, message = FALSE}

#creating a vector to subset data for values corresponding to funded and not funded
funding_vec <- c("expired","funded","fundraising","in_repayment","paid")

#subset data to values status column that belong to funding_vec vector.
funding_stat <- subset(kiva_ll, kiva_ll$status %in% funding_vec)

#creating a vector for values which are associated with funded loans
funded_vec <- c("paid","in_repayment","funded")

#create indicator variable for Funded or not funded loans
funding_stat <- funding_stat %>% 
  mutate(funding_status = if_else(funding_stat$status %in% funded_vec, "Funded","Not Funded"))

#Create dataframe with unique borrower id, use for loan and corresponding funding status
loan_use <- unique(funding_stat[ , c("borrower_image_id", "use","funding_status")])
```

We filter our dataset to these values and create our `funding_status` indicator variable. Next, we separate the data into two data frames - Funded loans and Not funded loans. We then perform the same text analysis steps as we did in the lenders section on the `use` column.

```{r borrower cleaning, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
Encoding(loan_use$use) <- "latin1" #To recognize special characters

funded_loan <- subset(loan_use,loan_use$funding_status == "Funded")
notfunded_loan <- subset(loan_use,loan_use$funding_status == "Not Funded")
#Subset data to relevant columns
# loan_use <- subset(loan_country, select=c("lender_id","borrower_image_id","use"))
# loan_use <- unique(loan_use[ , c("borrower_image_id", "use")])

funded_corpus_use <- Corpus(VectorSource(as.vector(funded_loan$use)))# Convert input data to a corpus
notfunded_corpus_use <- Corpus(VectorSource(as.vector(notfunded_loan$use)))

funded_corpus_use <- tm_map(funded_corpus_use, toSpace, "/")
funded_corpus_use <- tm_map(funded_corpus_use, toSpace, "@")
funded_corpus_use <- tm_map(funded_corpus_use, toSpace, "\\|")

notfunded_corpus_use <- tm_map(notfunded_corpus_use, toSpace, "/")
notfunded_corpus_use <- tm_map(notfunded_corpus_use, toSpace, "@")
notfunded_corpus_use <- tm_map(notfunded_corpus_use, toSpace, "\\|")


#Convert all text to lower case
funded_corpus_use <- tm_map(funded_corpus_use, content_transformer(tolower))
notfunded_corpus_use <- tm_map(notfunded_corpus_use, content_transformer(tolower))

# Remove  numbers
funded_corpus_use <- tm_map(funded_corpus_use, removeNumbers)
notfunded_corpus_use <- tm_map(notfunded_corpus_use, removeNumbers)

# Remove english common stopwords
funded_corpus_use <- tm_map(funded_corpus_use, removeWords, stopwords("english"))
notfunded_corpus_use <- tm_map(notfunded_corpus_use, removeWords, stopwords("english"))

# Remove punctuation
funded_corpus_use <- tm_map(funded_corpus_use, removePunctuation)
notfunded_corpus_use <- tm_map(notfunded_corpus_use, removePunctuation)

# Eliminate extra white spaces
funded_corpus_use <- tm_map(funded_corpus_use, stripWhitespace)
notfunded_corpus_use <- tm_map(notfunded_corpus_use, stripWhitespace)
```

```{r use matrix, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
# Build a term-document matrix
funded_TextDoc_dtm <- TermDocumentMatrix(funded_corpus_use)
notfunded_TextDoc_dtm <- TermDocumentMatrix(notfunded_corpus_use)

# Convert data type to matrix
funded_dtm_m <- as.matrix(funded_TextDoc_dtm)
notfunded_dtm_m <- as.matrix(notfunded_TextDoc_dtm)

# Sort by descending value of frequency
funded_dtm_v <- sort(rowSums(funded_dtm_m),decreasing=TRUE)
notfunded_dtm_v <- sort(rowSums(notfunded_dtm_m),decreasing=TRUE)

#Convert to dataframe
funded_dtm_d <- data.frame(word = names(funded_dtm_v),freq=funded_dtm_v)
notfunded_dtm_d <- data.frame(word = names(notfunded_dtm_v),freq=notfunded_dtm_v)

```

**Bar Chart**

First we plot two bar charts to visualize and compare the top 10 most frequently used words by borrowers whose loans have been funded and not funded.

COMMENT ON THESE BAR CHARTS: I SEE RICE, SUPPLIES, MERCHANDISE ON FUNDED BUT NOT FUNDED AND FOOD, CAPITAL, NEW ON NOT FUNDED BUT NOT FUNDED. THOSE FUNDED WORDS ARE MORE SPECIFIC TO ME, MAYBE LENDERS PREFER MORE SPECIFICITY. THEY WANT A BORROWER WHO HAS A CONCRETE PLAN, THEY DON'T WANT THEIR MONEY WASTED. KIND OF A REACH, BUT STARE AT THE DATA A LITTLE MORE AND TRY TO SAY SOMETHING. X-AXIS TEXT IS TOO SMALL, COULD ALSO ROTATE TEXT 45 OR 90 DEGREES OR DO AN AXIS FLIP. TITLE FORMAT IS A LITTLE AWKWARD WITH THE BRACKETS

```{r borrower barchart, echo=FALSE, out.width="50%", warning=FALSE, fig.show='hold'}
# Plot the most frequent words

par(mfrow=c(1,1)) 

#Borrower Bar Chart of top 10 frequent words
ggplot(funded_dtm_d[1:10,])+ aes(x = reorder(word, -freq), y=freq)+
       geom_bar(stat="identity", fill = "blue", alpha=.6,with = 0.5)+
  xlab("Word")+
  ylab("Frequency")+
  ggtitle("Top 10 most frequent words [Funded]")

ggplot(notfunded_dtm_d[1:10,])+ aes(x = reorder(word, -freq), y=freq)+
       geom_bar(stat="identity", fill = "blue", alpha=.6,with = 0.5)+
  xlab("Word")+
  ylab("Frequency")+
  ggtitle("Top 10 most frequent words [Not Funded]")

```

TABLE VARS SHOULD BE FORMATTED LIKE WORDS, NO UNDERSCORES

**Word Cloud**

WHICH IS WHICH

Similar to the bar charts we create two word clouds, one representing the most common words used by the borrowers whose Loans have been funded and the other representing loans that did not get funded. We plot all the words in the dataset which have a minimum frequency of 5. The results are not too different from the bar chart but this is a different way of analyzing text data and we also get to see more words that were used by the lenders for their motivation for lending.

FIND AT LEAST ONE DIFFERENCE AND COMMENT ON IT.

```{r, echo=TRUE, header=FALSE, message = FALSE, }
#Table to see no. of funded and not funded loans

funding_loans_count <- funding_stat %>% group_by(funding_status) %>% summarize(loan_count = length(unique(loan_id)))

funding_loans_count %>% kbl(caption = "Value Counts of funding status") %>% 
  kable_classic(bootstrap_options ="striped",full_width = F, html_font = "helvetica",position = "left")

```

```{r, echo=TRUE,header= FALSE,out.width="50%", message = FALSE, warning = FALSE, fig.show='hold'}
#Borrower Wordcloud
set.seed(1234)

par(mfrow=c(1,1)) 

wordcloud(words = funded_dtm_d$word, freq = funded_dtm_d$freq, min.freq = 5,
          max.words=100, main ="Wordcloud [Borrowers]",random.order=FALSE, rot.per=0.40,scale=c(4,0.75),
          colors=brewer.pal(8, "Dark2"))

wordcloud(words = notfunded_dtm_d$word, freq = notfunded_dtm_d$freq, min.freq = 5,
          max.words=100, main ="Wordcloud [Borrowers]",random.order=FALSE, rot.per=0.40,scale=c(4,0.75),
          colors=brewer.pal(8, "Dark2"))

```

From the bar chart we can see that the word with highest frequency is **buy** followed by **purchase**. This implies that most borrowers are applying for loans to be able to source the raw materials for their businesses. The remaining 8 words have less than half frequency than of the top 2 words. It is interesting to note that both funded and not funded loans have the same 2 words with the highest frequency.

Based on the word cloud we also observe that there is not a lot of difference in the kinds of words used by borrowers who fall into the Funded class as opposed to those in the Not funded class. However, we must also factor in that the number of loans that belong to Funded class is much higher than the loans belonging to the Not Funded class (This can be seen from the table).

HOW DOES THE NUMBER OF LOANS MATTER. FIND SOME DIFFERENCE TO COMMENT ON, EVEN IF IT'S MINOR

## Conclusion

In this section, we analyzed free text fields containing quotes from both lenders and borrowers to understand what motivates lenders. We observed that lenders are usually motivated by altruism and most commonly lend money to help developing nation entrepreneurs. When it comes to borrowers, we observe that most borrowers seek out loans in order to source raw materials or to purchase resources for their business or workplace. We also looked at a breakdown of Funded and Not funded loans for borrowers. While the differences are not stark between funded and non-funded borrowers, we observe that \[ADD SOME SMALL DIFFERENCE YOU OBSERVE AND SAY WHAT IT MIGHT SUGGESTION ABOUT LENDER PREFERENCES AND HENCE MOTIVATION\]. The words used by each type of borrower are very similar which could be indicative of the fact that there are other factors at play when it comes to getting a loan funded. SUCH AS? LIST OTHER FACTORS

CAREFUL WITH WORD CHOICE HERE: YOU CAN'T ACTUALLY OBSERVE LENDER MOTIVATION, WHAT YOU OBSERVE IS THAT THEIR SELF-REPORTRED REASON FOR LENDING OFTEN INVOLVES THE WORD HELP, AND THEN FROM THAT YOU COULD INFER...SAME FOR YOUR SENTENCE ABOUT BORROWERS, DISTINGUISH BETWEEN WHAT YOU ACTUALLY OBSERVE AND WHAT YOU ARE ASSUMING BASED ON THAT OBSERVATION.
