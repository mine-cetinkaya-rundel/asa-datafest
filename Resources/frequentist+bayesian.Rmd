---
title: "Logistic Regression"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
options(digits = 3)
```

ADD AN INTRODUCTION STATING WHAT PART OF THE PROMPT WE ARE CONSIDERING AND HOW YOU WILL DO IT, i.e. Frequentist and Bayesian logistic regression

Frequentism is statistical philosophy grounded in the interpretation of probability as the limit of its relative frequency in many trials. Probabilities can be found by a repeatable objective process. While a frequentist assumes that there are true values of the parameters of the model and computes the point estimates of the parameters, a Bayesian asserts that only data are real, and treats the model parameters as random variables whose uncertainty can be characterized by probability distributions.

WHY WOULD YOU USE BAYESIAN? PRIOR KNOWLEDGE, ETC. JUSTIFY WHY YOU ARE PRESENTING BOTH APPROACHES.

# Set Up

## Required Packages

First, we loaded all the packages needed to perform our analysis using the `library()` function:

SUMMARIZE THE PACKAGES WE'RE GOING TO USE BRIEFLY. AREN'T GGPLOT2 AND DPLYR IN TIDYVERSE?

```{r}
library(dplyr)
library(tidyverse)
library(ggplot2)
library(loo)
library(jtools)
library(countrycode)
library(pROC)
```

## Data set

Next, we read the data set we are performing our analysis on into the RStudio environment.

WHICH FILE ARE YOU USING AND WHY?

```{r}
loans <- read.csv("data/loans.csv")
```

------------- The EDA section below will not be shown on the tutorial ---------------

```{r eval=FALSE, echo=FALSE}
percent <- loans %>% 
  drop_na(sector) %>% # drop missing values by health variable
  group_by(sector) %>%  # specify categorical variable
  summarize(Frequency = n()) %>% # return counts / frequencies
  mutate(Percent = paste0(round(Frequency / nrow(loans) * 100, 2), "%"))

table <- kbl(percent, 
    caption = "Table 2: Frequency table for loan sectors") %>%
    kable_styling(bootstrap_options = "striped", full_width = FALSE, position = "left")
table
colourCount = length(unique(loans$sector))
getPalette = colorRampPalette(brewer.pal(12, "Set3"))
bar <- ggplot(data = percent, aes(x = reorder(sector, Frequency), y = Frequency)) +
  geom_col(aes(fill = getPalette(colourCount)) , show.legend = FALSE) +
  ggtitle(paste("Loan Sector Distribution")) +
  coord_flip() 
bar
```

```{r eval=FALSE, echo=FALSE}
status <- loans %>% 
  drop_na(status) %>% # drop missing values by health variable
  group_by(status) %>%  # specify categorical variable
  summarize(Frequency = n()) %>% # return counts / frequencies
  mutate(Percent = paste0(round(Frequency / nrow(loans) * 100, 2), "%"))

table1 <- tibble(status) %>% 
  arrange(desc(Percent))
table1
donut1 <- ggplot(status, aes(x = 2, y = Frequency, fill = status)) +
    geom_bar(stat = "identity") +
    coord_polar(theta = "y", start = 0) + 
    theme_void() + # these theme removes the lines around chart and grey background
    theme(legend.title = element_text(size = 12),
          legend.text = element_text(size = 12)) +
    scale_fill_brewer(palette="Accent") +
    labs(fill = "Status Distribution") +
    xlim(0.5, 2.5)
donut1
bar1 <- ggplot(loans, aes(x = status)) +
  geom_bar() +
  coord_flip()
bar1
```

------------- The EDA section above will not be shown on the tutorial ---------------

We then converted all $\text{NA}$ in the column `paid_amount` to 0 and created a new variable `continent` based on the country to reduce the number of levels of the geographical variable.

```{r}
loan <- loans %>% 
  mutate(paid_amount = if_else(is.na(paid_amount), 0, paid_amount),
         continent = countrycode(sourcevar = location.country,
                            origin = "country.name",
                            destination = "continent"))
```

------------- The EDA section below will not be shown on the tutorial ---------------

```{r eval=FALSE, echo=FALSE}
ggplot(data = loan, aes(x = funded_amount)) +
  geom_histogram(binwidth=100) +
  labs(title="Distribution of Funded Amount")

ggplot(data = loan, aes(x = paid_amount)) +
  geom_histogram(binwidth=100) +
  labs(title="Distribution of Paid Amount")

ggplot(loan, aes(x = posted_yr)) +
  geom_histogram()
```

```{r eval=FALSE, echo=FALSE}
loan %>%
  count(status, sector) %>%       
  group_by(status) %>%
  mutate(pct= prop.table(n) * 100) %>%
  ggplot() + aes(status, pct, fill=sector) +
  geom_bar(stat="identity") +
  labs(x = "Status", y = "Proportion") +
  coord_flip() 
```

```{r eval=FALSE, echo=FALSE}
#continent
ggplot(data=d, aes(x = fct_infreq(continent),  fill=continent)) +
  geom_bar(stat = 'count') +
  scale_fill_brewer(palette = "Accent") +
  labs(x = "Continent")

ggplot(data=d, aes(x = status)) +
  geom_bar(aes(fill=as.factor(continent)), position="fill") +
  scale_fill_brewer(palette = "Accent")

#sector
ggplot(data=d, aes(x = status)) +
  geom_bar(aes(fill=as.factor(sector)), position="fill") 

#loan_amount
ggplot(d, aes(x = loan_amount, group = status)) +
  geom_boxplot()
  #geom_boxplot(outlier.colour="red", outlier.shape=8, outlier.size=4)
```

------------- The EDA section above will not be shown on the tutorial ---------------

# Logistic Regression

SAY WHAT THE 8 LEVELS ARE, DESCRIBE WHAT A MULTINOMIAL LOGISTIC MODEL IS AND WHY IT'S APPROPRIATE.

The response of interest is loan payment status, recorded in the `status` column of the `lender` file. This is a categorical variable with 8 levels, combining non-funded, funded, incomplete and complete loans: STATE THE LEVELS. Intuitively, for a categorical response variable with at least three levels, one appropriate model would be a multinomial logistic regression. However, the interpretation of results is not as straightforward in such a model - and since the prompt is explicitly concerned with loan default, a more interpretable model would be one which dichotomizes the data to contrast defaulted and non-defaulted loans.

To achieve this, we restrict the dataset to completed loans, which are either repaid or not repaid. With a binary response variable, we can utilize a binary logistic regression. Note that in transforming the data, we set the `status` variable to be 1 if the loan status is "paid" and 0 if it is "defaulted", such that the baseline level would be "defaulted".

SAY HOW MANY OBSERVATIONS, WHAT PROPORTION YOU END UP DISCARDING DOING THIS AND COMMENT ON THAT. WHY DID YOU DECIDE TO CHOOSE THE BASELINE? THIS MEANS YOU WILL BE INTERPRETING COEFFICIENTS AS FEATURES ASSOCIATED WITH LOAN REPAYMENT, WHY DO YOU WANT TO DO THAT? THERE'S NOTHING WRONG, BUT JUSTIFY AND EXPLAIN IT.

```{r}
d <- loan %>% 
  filter(status == "paid"|status == "defaulted")
d <- d %>% 
  mutate(status = if_else(status == "paid", 1, 0))
```

SINCE THIS IS AN INTRO TUTORIAL, IT WOULD BE GOOD TO GIVE A LITTLE MORE INTUITION: EXPLAIN HOW A LINEAR REGRESSION WOULD GIVE NONSENSICAL VALUES AND NON-NORMAL ERRORS IF YOU APPLIED IT TO BERNOULLI DATA

The logistic regression model is an example of a broad class of models known as generalized linear models (GLM). GLM's extend ordinary least squares regression to response variables from arbitrary distributions through the use of a link function; instead of the response varying linearly with the covariates, its link function does. In this case, our response is a binary variable and so the Bernoulli model is appropriate. The Bernoulli distribution is a discrete probability distribution of a random variable which takes the value 1 with probability $p$ and the value 0 with probability $q=1-p$.The likelihood for one observation $y$ can be written as a Bernoulli PMF over possible outcomes $k$: $$\begin{equation}
    f(k;p) =
    \left\{
        \begin{array}{cc}
                p & \mathrm{if\ } k=1 \\
                1-p & \mathrm{if\ } k=0 \\
        \end{array} 
    \right.
\end{equation},$$

The two most common link functions used for binomial GLMs are the logit and probit functions. For binary outcomes a common choice is the *logit link.* The logit link converts probabilities to log odds, and therefore the model coefficients can be easily interpreted in terms of odds ratios. When the logit link function is used the model is often referred to as a logistic regression model (the inverse logit function is the CDF of the standard logistic distribution).

## Frequentist Approach

We first fit the logistic regression model using a Frequentist approach.

### Variable Selection

YOU AREN'T USING STATUS ANYMORE, YOU'RE USING A TRANSFORMED VARIABLE, IT SHOULD HAVE A DIFFERENT NAME.

WHERE DID YOU GET THE IDEA FOR THESE? YOU VERY BRIEFLY SHOW SOME EDA TO JUSTIFY THESE OR REFERENCE THE SPECIFIC PARTS OF JENNY'S EDA THAT INSPIRED THESE CHOICES (I.E., SAY WHAT TRENDS YOU SAW). EXPLAIN EACH VAR SEPARATELY, DON'T GROUP SECTOR, YEAR, CONTINENT TOGETHER

The response variable we were using is `status,` since we wanted to explore which factors are affecting the loan repayment. For the predictors, we are interested in the following variables:

-   `loan_amount`, because we think that the smaller loans tend to be more easily paid back than the larger ones.

-   `sector`, `posted_yr`, `continent,`as our EDA shows the status distribution of loan differs across sectors, the year when the loan is posted on Kiva, and the continent where the borrower is located.

-   `time`, which is a variable we created representing the time from when the loan is posted to when the loan is funded, in months, since sometimes the time it takes for the borrowers to receive the loan may affect their financial situation and thus affect their ability to repay the loan according to **t**he terms agreed.

-   `dif`, which is a variable we created to calculate the difference between the loan amount and funded amount, because whether the borrowers get the full amount of loan they requested may affect their financial ability to pay back the loan.

-   `sex`, which indicates the sex of a borrower group, where "mixed" means there are both males and females in a borrower group.

```{r}
d <- d %>% 
  select(loan_id, loan_amount, status, funded_amount, posted_yr, posted_mo, posted_day, funded_yr, funded_mo, funded_day, sector, continent, borrower_m_count, borrower_f_count) %>% 
  mutate(dif = loan_amount-funded_amount,
         time = (funded_yr - posted_yr)*365+(funded_mo - posted_mo)*30 + (funded_day - posted_day),
         sex = case_when(borrower_m_count == 0 ~ "female",
                         borrower_f_count == 0 ~ "male",
                         TRUE ~ "mixed"))
```

```{r}
unique(d$dif)
```

A closer look at the variable `dif` showed that after we filtered the original data set to contain only completed loans, all the observations have funded amount equal to the loan amount. Thus, the variable `dif` would not make any difference on the response variable, so we were excluding it from our model.

IS DIF NONZERO IN THE FULL DATA? IF SO MENTION IT, IF NOT JUST CUT IT.

### Data Splitting

Next, we split our data into training set and test set, in order to estimate the performance of our model on new data: data not used to train.

EXPLAIN MORE, WHAT'S THE PHILOSOPHY OF DATA SPLITTING, TALK ABOUT OVERFITTING, ETC.

```{r}
seed <- 2
train1 <- sample(d$loan_id, 30000)
training <- d %>% 
  filter(loan_id %in% train1)
testing <-d %>% 
  filter(!loan_id %in% train1)
```

### Model Fitting

Now we're ready to fit our logistic model on the training set using the `glm()` function:

```{r}
model <- glm(status ~ loan_amount + sector + posted_yr + continent + time + sex, data = training, family = "binomial"(link = "logit")) 
```

Here we specify the family to be binomial and the link function to be logit so that the function impliments a logistic regression, described above.

```{r eval=FALSE, echo=FALSE}
modell <- glm(status ~ loan_amount + sector + as.factor(posted_yr) + continent + time + sex, data = training, family = "binomial"(link = "logit"))

# compare two models with different types for the posted_yr variable
summ(model)
summ(modell)
```

### Model Selection

To identify variables that are important in explaining variation in the response, we perform model selection using the `step()` function. Specifying "direction="both"" tells R to perform both forward and backward selections. The default selection criterion is $\mathrm{AIC}$: from the current model, it drops or adds the one variable that leads to the best $\mathrm{AIC}$ improvement (smallest $\mathrm{AIC}$).

SAY WHAT FORWARD AND BACKWARD SELECTION ARE.

```{r}
step(model, direction="both",trace=FALSE)
```

The variables selected here are all except `time`. We dropped the `time` variable from our model. The final model is then:

```{r}
final <- glm(status ~ loan_amount + sector + posted_yr + continent + sex, data = training, family = "binomial"(link = "logit")) 
```

### Model Output

We now look at a summary of the model output using the `summ()` function from the `jtools` package:

```{r}
summ(final)
```

COMMENT ON WHICH COEFFICIENTS ARE LARGEST AND MOST SIGNIFICANT.

In order to evaluate our model results, we first look at coefficients that are large in magnitude and statistically significant...YOU NEED TO EXPLAIN P-VALUES.

From the p-values of model coefficients, the significant predictors are `loan_amount`, `sector`, `posted_yr`, and `continent`, which have p-values less than 0.05. The variable `sex` can also be considered significant if we use a significance level of 0.1 instead of 0.05. Here we can notice that the coefficient of `loan_amount` is about 0, meaning that the odds of loan repayment is expected to be almost the same across all values of loan amount, which indicates the loan amount has little effect on whether the loan is paid back or not. Also, the p-value is extremely close to 0, indicating a strong statistical significance of this predictor.

Interpreting the coefficients in a logistic regression is a little more complicated than in linear regression, but still quite straight-forward. For example, the coefficient for `posted_yr` is "0.42", which means that for a given year when the loan is posted, the expected odds of the loan getting repaid is $e^{0.42}\approx1.52$ times the odds of the loan getting repaid if the loan is posted in the previous year.

HOW DO YOU INTERPRET THAT IN THE CONTEXT OF THE PROMPT? DISCUSS THE MOST NOTEWORTHY COEFFICIENTS HERE.

### Model Evaluation

DON'T USE THE TERM CONFUSION MATRIX WITHOUT DEFINING IT. SHOW THE CONFUSION MATRIX.

To evaluate our model, we the ROC (*Receiver Operating Characteristic*) curve to evaluate different thresholds for a classification problem. In a nutshell, ROC curve visualizes a confusion matrix for every threshold. ROC curve shows a *False positive rate* on the X-axis. This metric informs us about the proportion of the negative class classified as positive. We used `ggroc()` from the `PROC` package to plot the ROC curve.

```{r}
predicted1 <- predict(final, newdata = testing)
roc1 = roc(response = testing$status, predictor = predicted1)
ggroc(roc1, legacy.axes = TRUE) +
  labs(x = 'False-positive rate', y = 'True-positive rate', title = 'Simulated ROC curve')
```

```{r eval=FALSE, echo=FALSE}
predicted11 <- predict(modell, newdata = testing)
roc11 = roc(response = testing$status, predictor = predicted11)
ggroc(roc11, legacy.axes = TRUE) +
  labs(x = 'False-positive rate', y = 'True-positive rate', title = 'Simulated ROC curve')
```

On the Y-axis, it shows a *True positive rate.* This metric is sometimes called *Recall* or *Sensitivity*. It shows the positive class proportion that was correctly classified.

AUC represents the area under the ROC curve. AUC ranges in value from 0 to 1. A model whose predictions are 100% wrong has an AUC of 0; one whose predictions are 100% correct has an AUC of 1. Ideally, the ROC curve should extend to the top left corner. The AUC score would be 1 in that scenario. We used the `auc()` function from the `pROC` package to calculate the AUC of the model.

```{r}
auc(testing$status,predicted1)
```

```{r eval=FALSE, echo=FALSE}
auc(testing$status,predicted11)
```

Here we got an AUC score of 0.7158, which means there is about 70% chance that our model will be able to distinguish between positive class (paid) and negative class (defaulted). In general, an AUC of 0.5 suggests no discrimination, 0.7 to 0.8 is considered acceptable, 0.8 to 0.9 is considered excellent, and more than 0.9 is considered outstanding.

### Conclusion

YOU DISCUSS ALL OF THESE RESULTS AS THOUGH YOU ARE DOING DESCRIPTIVE STATISTICS, NOT RUNNING A MODEL. YOU SHOULD BE TALKING ABOUT THE EFFECT OF CHANGES IN THE COEFFICIENTS ON THE ODDS OF REPAYMENT ALL ELSE EQUAL, NOT JUST "AFRICA HAS THE LOWEST..."

Based on our analysis, loan amount seems not to be a factor affecting the loan repayment status at all. Loans in the Education sector are the most likely to be repaid, while loans in the Personal Use sector are the least likely to be repaid. In terms of the continent where the loan transaction took place, Oceania has the highest odds ratio for the loan being repaid while Africa has the lowest odds ratio. As for sex of the borrower(s), a borrower group that consists of both males and females has the highest odds ratio of repaying the loan, and a female borrower or a female-only borrower group generally has higher odds ratio of repaying the loan than a male borrower or a male-only borrower group.

## Bayesian Approach

Next, we implement an alternative to the model above using a Bayesian approach. The four steps of a Bayesian analysis are:

1)  Specify a joint distribution for the outcome(s) and all the unknowns, which typically takes the form of a marginal prior distribution for the unknowns multiplied by a likelihood for the outcome(s) conditional on the unknowns. This joint distribution is proportional to a posterior distribution of the unknowns conditional on the observed data
2)  Draw from posterior distribution using Markov Chain Monte Carlo (MCMC).
3)  Evaluate how well the model fits the data and possibly revise the model.
4)  Draw from the posterior predictive distribution of the outcome(s) given interesting values of the predictors in order to visualize how a manipulation of a predictor effects (a function of) the outcome(s).

CORRECT 2., YOU DON'T ALWAYS HAVE TO USE MCMC, E.G. CONJUGATE PRIORS. ARE THESE STEPS QUOTED FROM SOMEWHERE? IF SO CITE.

To fit our model, we will use the same data split as in the Frequentist analysis above.

IS THERE SOME REASON YOU NEED TO RELOAD THE DATA AND RESPLIT THE DATA? IT WOULD BE BETTER TO USE THE SAME SPLIT AS ABOVE. DELETE DATA SPLITTING IF THAT WORKS.

### Data Splitting

First, we set a seed so that we get the same results for randomization. Then, to reduce time-complexity that `stan_glm()` needs to fit a model, we sampled 5000 from the 68707 observations.

```{r}
seed <- 1
d1 <- sample_n(d, 5000)
```

```{r include=FALSE}
load("data/post1.Rdata")
load("data/post0.Rdata")
```

### Model Fitting

To implement our Bayesian logistic model, we utilize an algorithm known as Hamiltonian Monte Carlo. In particular, we use the program Stan, via its $\texttt{R}$ interface in the $\texttt{rstanarm}$ package.

FIX ALL OF YOUR PACKAGE TEXT- THIS IS THE THIRD TIME I'VE HAD TO ASK THIS, FIXING THE FIRST TWO FOR YOU HERE BUT PLEASE CHECK THE REST OF YOUR DOCUMENT. WHAT ARE YOUR PRIORS? IT'S ODD TO SAY THAT YOU NEED TO SELECT THEM AND THEN NOT MENTION HOW YOU SELECT THEM. IF YOU USE DEFAULT PRIORS LOOK UP WHAT THEY ARE AND MENTION IT. SHOW HOW TO VIEW THE PRIORS FOR A MODEL using prior_summary()

The `rstanarm` equivalent of g`lm()` is `stan_glm()`, which supports every link function that `glm()` supports. Rather than generate point estimates and a covariance, Stan instead **simulates draws from the posterior distribution of our parameters**. The Stan output is a matrix in which each row is a simulated draw from the posterior distribution OF WHAT and each column is a parameter (or function of parameters). With `stan_glm()`, binomial models with a logit link function can typically be fit slightly faster than the identical model with a probit link.

Note that in contrast with the Frequentist procedure, in Bayesian estimation we need to specify priors for our parameters, which permit us to incorporate existing knowledge of the parameters into our model. In this case.... FINISH SENTENCE

In the following code, we specified the chosen link, and set priors for the intercept and the predictor coefficients.

WHY ARE YOU USING NORMAL (0,1)? WHAT IS THE SCALE OF THE PREDICTORS? THAT COULD BE VERY INFORMATIVE. MAYBE NOT BUT YOU NEED TO JUSTIFY IF YOU ARE USING SOMETHING OTHER THAN THE DEFAULT PRIORS (WHICH SCALE THE PREDICTORS TO GET AROUND THIS)

```{r eval=FALSE}
post1 <- stan_glm(status ~ loan_amount + sector + posted_yr + continent + sex, data = d1,
                 family = binomial(link = "logit"), 
                 prior = normal(0,1), prior_intercept = normal(0,1),
                 seed = seed,
                 refresh = 0,
                 cores=3, chains = 3,warmup = 500,iter = 1200)
```

We can view the output of the model here:

```{r}
post1
```

YOU SAID ABOVE IT DOESN'T PRODUCE POINT ESTIMATES - CLARIFY.

It includes the point estimates (median) as well as the uncertainty estimates (median absolute deviation) for coefficients of all predictors. Here, the coefficients differ from those in the previous model with frequentist approach: again we can notice that the median estimate for `loan_amount` is 0.0 with a median absolute deviation of 0.0, indicating no relationship between loan amount and the loan status; the loans in the "Health" sector become the most likely to be repaid, and loans in the "Services" sector is become the least likely to be repaid. Together with Oceania, Europe now has the same highest odds ratio for the loan being repaid while Africa still has the lowest odds ratio. Contrary to what our frequentist model suggests ([**here**](link%20to%20be%20added%20after%20embedded%20in%20the%20website)), a borrower group that consists of both males and females has the lowest odds ratio of repaying the loan, and a female borrower or a female-only borrower group generally has lower odds ratio of repaying the loan than a male borrower or a male-only borrower group.

FIX THIS DISCUSSION, SAME FEEDBACK AS ABOVE FOR FREQUENTIST

ADD DISCUSSION OF CREDIBLE INTERVALS AND DEFINE, SINCE YOU USE THE CONCEPT BELOW.

### Model Parameter Visualization

In order to get an idea of which variables are associated with changes in the probability of repayment, we draw a caterpillar plot, which displays the median and estimate interval for all the coefficients.

First, we sort the predictor variables in our model `post1` by coefficients. SORT BY POSTERIOR MEDIAN OF COEFFICIENTS? THIS IS UNCLEAR. To do this, we extracted the `coefficient` element from `post1` and converted it into a data frame which had only one column "post1.coefficients". We then sorted the rows based on the value of the coefficients. Next, we extracted the row names except for the first row (the intercept) and stored it for use in drawing the caterpillar plot. CONFUSING AND WORDY, TIGHTEN THIS UP. ARE YOU SORTING THE POSTERIOR DRAWS OR A POINT ESTIMATE?

This caterpillar plot is created by the `mcmc_intervals()` function in the `bayesplot` package. It plots the 95% uncertainty intervals computed from posterior draws with all chains merged.

```{r}
# extract the coefficients and sort them in ascending order
coef <- data.frame(post1$coefficients) %>% 
  arrange(post1.coefficients)

# store the variables corresponding to the coefficients as a list
order <- rownames(coef)[-1]

# plot the sorted coefficients with uncertainty interval
cate <- mcmc_intervals(as.matrix(post1), pars = order, prob = 0.95, prob_outer = 1)
cate
```

### Model Evaluation

```{r}
(loo1 <- loo(post1, save_psis = TRUE))
```

In the code chunk above, we assessed the strength of our model via its posterior predictive LOOCV (Leave One Out Cross-Validation). Cross-validation is a technique for evaluating models by training the models on subsets of the available input data and evaluating them on the complementary subset of the data. LOOCV is a type of cross-validation approach in which each observation is considered as the validation set and the rest ($N-1$) observations are considered as the training set. PSIS-LOO CV is and efficient approximate LOOCV for Bayesian models using Pareto smoothed importance sampling ([**PSIS**](https://www.rdocumentation.org/link/PSIS?package=loo&version=2.5.1&to=%3Dpsis)). We can see from the output that PSIS-LOO result is reliable as all Pareto k estimates are small (k\< 0.5). However as we know, this accuracy rate is quite meaningless unless we have something to compare it to. So we created a baseline model with no predictors for comparison:

```{r eval=FALSE}
post0 <- stan_glm(status ~ 1, data = d1,
                 family = binomial(link = "logit"), 
                 prior = normal(0,1), prior_intercept = normal(0,1),
                 seed = seed,
                 refresh = 0,
                 cores=3, chains = 3, warmup = 500,iter = 1200)
```

```{r}
(loo0 <- loo(post0, save_psis = T))
rstanarm::compare_models(loo0, loo1)
```

Since difference is positive, the expected predictive accuracy for the second model (post1) is higher.

Below, we compute posterior predictive probabilities of the linear predictor via the `posterior_linpred()` function provided in the `rstanarm` package. This function will extract posterior draws from the linear predictor. If we used a link function, then specifying the transform argument as `TRUE` will return the predictor as transformed via the inverse-link.

```{r}
preds <- posterior_linpred(post1, transform=TRUE)
pred <- colMeans(preds)
```

We calculate these posterior predictive probabilities in order to determine the classification accuracy of our model. If the posterior probability of paying back the loan for an borrower is greater or equal to 0.5, then we would predict that observation to be a repaid loan (and similarly for less than 0.5). For each observation, we can compare the posterior prediction to the actual observed value. The proportion of times we correctly predict an individual (i.e. \[prediction = 0 and observation = 0\] or \[prediction = 1 and observation = 1\]) is our classification accuracy.

```{r}
pr <- as.integer(pred >= 0.5)
round(mean(xor(pr,as.integer(d1$status==0))),3) #classification accuracy
```

We should also evaluate the classification accuracy of our model on unseen data to see whether it is generalized or overfitting. This can be done by using a test dataset or via a LOOCV approach. Since we've talked about the former approach in the frequentist section ([**here**](link%20to%20be%20added%20after%20embedded%20in%20the%20website)), in this section we would use the latter to illustrate the function `E_loo()`, which uses importance weights generated from the `loo()` function.

```{r}
ploo = E_loo(preds, loo1$psis_object, type = "mean", log_ratios = -log_lik(post1))$value
round(mean(xor(ploo > 0.5, as.integer(d1$status == 0))),3)
```

The accuracy is 0.976, which is very high. However, this does not necessarily mean our model is doing great in prediction. Instead, it is probably just due to the high unbalance of the data, with almost all observations having the value "paid" for the response variable.

```{r}
qplot(pred, ploo,geom=c("point", "smooth")) +
  ggtitle("LOO probability vs. posterior predictive probability")
```

In the plot above we can see the small difference in posterior predictive probabilities and LOO probabilities.

We can again use ROC and AUC to evaluate this model. First, we need to prepare a test data set. Here we are extracting data from the original data set that are not in the sample.

```{r}
train <- d1
test <- setdiff(d, train)
predicted <- predict(post1, newdata=test)
```

Here's the ROC curve of our model on test data:

```{r}
roc = roc(response = test$status, predictor = predicted)
ggroc(roc, legacy.axes = TRUE) +
  labs(x = 'False-positive rate', y = 'True-positive rate', title = 'Simulated ROC curve')
```

```{r}
auc(test$status,predicted)
```

Here we got an AUC score of 0.7019, which means there is about 70% chance that our model will be able to distinguish between positive class (paid) and negative class (defaulted).

Also, we can save our models in a folder so that RStudio doesn't need to go through the model fitting process every time we knit the document. Just load the models from where they are stored.

```{r eval=FALSE, echo=FALSE}
save(post1, file = "data/post1.RData")
save(post0, file = "data/post0.RData")
```

WHERE IS YOUR CONCLUSION? COMPARE THIS TO FREQUENTIST. DO YOU USE YOUR DATA SPLITTING?
