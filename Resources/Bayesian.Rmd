---
title: "Bayesian Logistic Regression"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
options(digits = 3)
```

# Set Up

We loaded the packages, read in the data set, and manipulated the data as we did in the frequentist logistic regression section [**here**](link to be added after embedded in the website)

```{r include=FALSE}
loans <- read.csv("data/loans.csv")
```

```{r}
library(dplyr)
library(tidyverse)
library(ggplot2)
library(loo)
library(jtools)
library(countrycode)
library(rstanarm)
library(bayesplot)
library(pROC)

loan <- loans %>% 
  mutate(paid_amount = if_else(is.na(paid_amount), 0, paid_amount),
         continent = countrycode(sourcevar = location.country,
                            origin = "country.name",
                            destination = "continent"))

d <- loan %>% 
  filter(status == "paid"|status == "defaulted") %>% 
  mutate(status = if_else(status == "paid", 1, 0))

d <- d %>% 
  select(loan_id, loan_amount, status, funded_amount, posted_yr, posted_mo, posted_day, funded_yr, funded_mo, funded_day, sector, continent, borrower_m_count, borrower_f_count) %>% 
  mutate(dif = loan_amount-funded_amount,
         time = (funded_yr - posted_yr)*12+(funded_mo - posted_mo),
         sex = case_when(borrower_m_count == 0 ~ "female",
                         borrower_f_count == 0 ~ "male",
                         TRUE ~ "mixed"))
```

# Bayesian Approach

While a frequentist assumes that there are true values of the parameters of the model and computes the point estimates of the parameters, a Bayesian asserts that only data are real, and treats the model parameters as probability distributions which are to be inferred.

The four steps of a Bayesian analysis are:

1)  Specify a joint distribution for the outcome(s) and all the unknowns, which typically takes the form of a marginal prior distribution for the unknowns multiplied by a likelihood for the outcome(s) conditional on the unknowns. This joint distribution is proportional to a posterior distribution of the unknowns conditional on the observed data
2)  Draw from posterior distribution using Markov Chain Monte Carlo (MCMC).
3)  Evaluate how well the model fits the data and possibly revise the model.
4)  Draw from the posterior predictive distribution of the outcome(s) given interesting values of the predictors in order to visualize how a manipulation of a predictor affects (a function of) the outcome(s).

To run this model using Bayesian estimation, we can use the `rstanarm` package. This package wraps Stan code for common regression models. In contrast with the frequentist procedure, in Bayesian estimation we need to specify priors for our parameters. The `rstanarm` function equivalent of g`lm()` is `stan_glm()`, which supports every link function that `glm()` supports. Rather than generate point estimates and a covariance, Stan instead **simulates draws from the posterior distribution of our parameters** using a sophisticated algorithm called Hamiltonian Monte Carlo. Stan output is a matrix in which each row is a simulated draw from the posterior and each column is a parameter (or function of parameters). With `stan_glm()`, binomial models with a logit link function can typically be fit slightly faster than the identical model with a probit link because of how the two models are implemented in Stan. In the following code, we simply specify the chosen link, and set priors for the intercept and the predictor coefficients.

## Data Splitting

First, we set a seed so that we get the same results for randomization. Then, to reduce time-complexity that `stan_glm()` needs to fit a model, we sampled 5000 from the 68707 observations.

```{r}
seed <- 1
d1 <- sample_n(d, 5000)
```

```{r include=FALSE}
load("data/post1.Rdata")
load("data/post0.Rdata")
```

## Model Fitting

In the following code, we specified the chosen link, and set priors for the intercept and the predictor coefficients.

```{r eval=FALSE}
post1 <- stan_glm(status ~ loan_amount + sector + posted_yr + continent + sex, data = d1,
                 family = binomial(link = "logit"), 
                 prior = normal(0,1), prior_intercept = normal(0,1),
                 seed = seed,
                 refresh = 0,
                 cores=3, chains = 3,warmup = 500,iter = 1200)
```

Here's the output of the model:

```{r}
post1
```

It includes the point estimates (median) as well as the uncertainty estimates (median absolute deviation) for coefficients of all predictors. Here, the coefficients differ from those in the previous model with frequentist approach: again we can notice that the median estimate for `loan_amount` is 0.0 with a median absolute deviation of 0.0, indicating no relationship between loan amount and the loan status; the loans in the "Health" sector become the most likely to be repaid, and loans in the "Services" sector is become the least likely to be repaid. Together with Oceania, Europe now has the same highest odds ratio for the loan being repaid while Africa still has the lowest odds ratio. Contrary to what our frequentist model suggests ([**here**](link to be added after embedded in the website)), a borrower group that consists of both males and females has the lowest odds ratio of repaying the loan, and a female borrower or a female-only borrower group generally has lower odds ratio of repaying the loan than a male borrower or a male-only borrower group.

## Model Parameter Visualization

Next, we drew a caterpillar plot, which displays the median and estimate interval for all the coefficients sorted by their medians so that we can get a closer look at which variables are expected to have greater effects on the response variable. First, we sorted the predictor variables in our model `post1` by coefficients. To do this, we extracted the `coefficient` element from `post1` and converted it into a data frame which had only one column "post1.coefficients". We then sorted the rows based on the value of the coefficients. Next, we extracted the row names except for the first row (the intercept) and stored it for use in drawing the caterpillar plot.

```{r}
# extract the coefficients and sort them in ascending order
coef <- data.frame(post1$coefficients) %>% 
  arrange(post1.coefficients)

# store the variables corresponding to the coefficients as a list
order <- rownames(coef)[-1]

# plot the sorted coefficients with uncertainty interval
cate <- mcmc_intervals(as.matrix(post1), pars = order, prob = 0.95, prob_outer = 1)
cate
```

This caterpillar plot is created by the `mcmc_intervals()` function in the `bayesplot` package. It plots the 95% uncertainty intervals computed from posterior draws with all chains merged. The intervals are sorted by the median of each coefficient, and the ordering gives us a more direct sense of which variables have greater effects on the loan repayment status.

### Model Evaluation

```{r}
(loo1 <- loo(post1, save_psis = TRUE))
```

In the code chunk above, we assessed the strength of our model via its posterior predictive LOOCV (Leave One Out Cross-Validation). Cross-validation is a technique for evaluating models by training the models on subsets of the available input data and evaluating them on the complementary subset of the data. LOOCV is a type of cross-validation approach in which each observation is considered as the validation set and the rest ($N-1$) observations are considered as the training set. PSIS-LOO CV is and efficient approximate LOOCV for Bayesian models using Pareto smoothed importance sampling ([**PSIS**](https://www.rdocumentation.org/link/PSIS?package=loo&version=2.5.1&to=%3Dpsis)). We can see from the output that PSIS-LOO result is reliable as all Pareto k estimates are small (k\< 0.5). However as we know, this accuracy rate is quite meaningless unless we have something to compare it to. So we created a baseline model with no predictors for comparison:

```{r eval=FALSE}
post0 <- stan_glm(status ~ 1, data = d1,
                 family = binomial(link = "logit"), 
                 prior = normal(0,1), prior_intercept = normal(0,1),
                 seed = seed,
                 refresh = 0,
                 cores=3, chains = 3, warmup = 500,iter = 1200)
```

```{r}
(loo0 <- loo(post0, save_psis = T))
rstanarm::compare_models(loo0, loo1)
```

Since difference is positive, the expected predictive accuracy for the second model (post1) is higher.

Below, we compute posterior predictive probabilities of the linear predictor via the `posterior_linpred()` function provided in the `rstanarm` package. This function will extract posterior draws from the linear predictor. If we used a link function, then specifying the transform argument as `TRUE` will return the predictor as transformed via the inverse-link.

```{r}
preds <- posterior_linpred(post1, transform=TRUE)
pred <- colMeans(preds)
```

We calculate these posterior predictive probabilities in order to determine the classification accuracy of our model. If the posterior probability of paying back the loan for an borrower is greater or equal to 0.5, then we would predict that observation to be a repaid loan (and similarly for less than 0.5). For each observation, we can compare the posterior prediction to the actual observed value. The proportion of times we correctly predict an individual (i.e. [prediction = 0 and observation = 0] or [prediction = 1 and observation = 1]) is our classification accuracy.

```{r}
pr <- as.integer(pred >= 0.5)
round(mean(xor(pr,as.integer(d1$status==0))),3) #classification accuracy
```

We should also evaluate the classification accuracy of our model on unseen data to see whether it is generalized or overfitting. This can be done by using a test dataset or via a LOOCV approach. Since we've talked about the former approach in the frequentist section ([**here**](link to be added after embedded in the website)), in this section we would use the latter to illustrate the function `E_loo()`, which uses importance weights generated from the `loo()` function.

```{r}
ploo = E_loo(preds, loo1$psis_object, type = "mean", log_ratios = -log_lik(post1))$value
round(mean(xor(ploo > 0.5, as.integer(d1$status == 0))),3)
```

The accuracy is 0.976, which is very high. However, this does not necessarily mean our model is doing great in prediction. Instead, it is probably just due to the high unbalance of the data, with almost all observations having the value "paid" for the response variable.

```{r}
qplot(pred, ploo,geom=c("point", "smooth")) +
  ggtitle("LOO probability vs. posterior predictive probability")
```

In the plot above we can see the small difference in posterior predictive probabilities and LOO probabilities.

We can again use ROC and AUC to evaluate this model. First, we need to prepare a test data set. Here we are extracting data from the original data set that are not in the sample.

```{r}
train <- d1
test <- setdiff(d, train)
predicted <- predict(post1, newdata=test)
```

Here's the ROC curve of our model on test data:

```{r}
roc = roc(response = test$status, predictor = predicted)
ggroc(roc, legacy.axes = TRUE) +
  labs(x = 'False-positive rate', y = 'True-positive rate', title = 'Simulated ROC curve')
```

```{r}
auc(test$status,predicted)
```

Here we got an AUC score of 0.7019, which means there is about 70% chance that our model will be able to distinguish between positive class (paid) and negative class (defaulted).

Also, we can save our models in a folder so that RStudio doesn't need to go through the model fitting process every time we knit the document. Just load the models from where they are stored.

```{r eval=FALSE, echo=FALSE}
save(post1, file = "data/post1.RData")
save(post0, file = "data/post0.RData")
```
