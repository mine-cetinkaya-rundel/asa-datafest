---
title: "Your First DataFest Event"
date: '2022-07-01'
output: 
  html_document: 
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

It's 6pm on the day of your first DataFest and you've just received your assignment: in approximately 48 hours, you will need to have cleaned your dataset, analyzed it, and collected your findings into a digestible slide format. This might seem daunting, especially if you're new to data analysis. But rest assured, with a little preparation DataFest can be a great experience - even for someone with limited statistical background. Here, we break the process down into several approachable steps by walking through one potential analysis on the 2012 DataFest dataset. By the end of this tutorial, you'll be fully equipped to dive into your DataFest journey.

## Preparation Phase

### Setting up R

First, you're going to need to choose a language and software. Python and R are two of the most popular choices for Data Science. For this tutorial, we'll be working in R.

To download R, go to this [link](https://cloud.r-project.org/) and choose the correct download option.

You should also [download](https://www.rstudio.com/products/rstudio/download/) RStudio. Once you've finished the download and opened RStudio, your IDE will look something like this:

![](data/start_page.png)

Open an R Markdown file by clicking on File -\> New File -\> R Markdown.

![](data/rscript.png)

This will open a file that looks like this:

![](data/rmarkdown.png)

This file will include some basic starter code examples, which you can delete and replace with your own.

If you'd like to see the rendered version, click the knit button. The knitted starter code looks like this:

![](data/knitted.png)

For more in depth help with R and R Markdown, check out these resources:

-   [R for Data Science textbook](https://r4ds.had.co.nz/)
-   [R resource](https://cran.r-project.org/doc/contrib/Paradis-rdebuts_en.pdf)
-   [R Markdown tutorial](https://www.rstudio.com/resources/webinars/getting-started-with-r-markdown/)

### Collaborative Data Science

Since you'll be working in a collaborative setting for DataFest, you'll want to find a workflow that accommodates your entire team's needs. One great tool for this is GitHub, a code hosting platform for version control and collaboration. GitHub allows you to create a code repository shared among different collaborators. Everyone is able to make changes to the project and access each others changes.

This link offers an in-depth walkthrough on GitHub if you'd like more information: [GitHub walkthrough](https://docs.github.com/en/get-started/quickstart/hello-world)

For a project like this, it's a good idea to have everyone write individual code files with their assigned tasks. This can help minimize merge conflicts, which can take a lot of time to troubleshoot when files have multiple authors. It's also good practice to comment your code so your teammates can understand it. Make sure to knit frequently to share your progress as well!

## Getting Started

Now that you're all set up, you can assess the prompt and data. In this tutorial, we'll be using the dataset from the 2012 DataFest event.

**The dataset:** In 2012, DataFest participants were asked to analyze a dataset provided by the San Francisco-based nonprofit, Kiva. Kiva is a microfinance organization which connects online lenders to entrepreneurs across the globe that might otherwise be excluded from conventional credit markets; this model is known as peer-to-peer lending. The aim is to expand the financial resources available in underserved communities.

**The prompt:** *Help understand what motivates people to lend money to developing-nation entrepreneurs and what factors are associated with paying these loans.*

Before beginning, it's important to note that the 2012 DataFest dataset is not an exhaustive record of all Kiva borrowers, but a subset selected for the purposes of the competition. In order to proceed with your analysis, you must assume that the data provided is a representative sample. However, note that if this assumption is violated, i.e., if the data differs systematically from the full population of Kiva transactions due to selection bias, your inferences will not be valid. Make sure to note this when you present your results.

To begin, first take stock of the information provided: the dataset involves 4 main files and 2 supplementary files, as well as a *data dictionary* - the data dictionary is a great place to start! A data dictionary is a collection of names, definitions, and attributes about data elements that are being used or captured in a database, information system, or part of a research project. It describes the meanings and purposes of data elements within the context of a project, and provides guidance on interpretation, accepted meanings and representation.

This is what the Kiva data dictionary looks like:

![](data/datadict.png)

Taking into account the outlined goal and provided data, you'll want to ask yourself what you intend to present by the end of your 48 hours. What questions do you want to answer? What visualizations will be appropriate?

At this step, browse the data to gain an understanding of what resources are available and what analysis angles might make sense.

First let's import some libraries. These libraries allow you to access a broad range of data analysis tools not included in the base R package.

\*Make sure to call `install.packages("package_name")` in the console if you are getting package issues!

```{r, message=FALSE, warning=FALSE, results='hide'}
library("tidyverse")
```

Now, let's import the data. Make sure you've downloaded the data into the proper directory, then import it using the appropriate file path.

```{r, message=FALSE, warning=FALSE, results = 'hide'}
kiva_ll <- read_csv(("data/lender_loans.csv"))
kiva_lender <- read_csv(("data/lenders.csv"))
kiva_loan <- read_csv(("data/loans.csv"))
kiva_partner <- read_csv(("data/partners.csv"))
```

We can use the `head()` command to take a look at the downloaded data:

```{r}
head(kiva_loan)
```

For a clearer view of the entire dataframe, we can also open it up in Excel or any csv file viewer:

![](data/kiva_loans.png)

At a glance, we can see columns like `status`, `activity`, and `sector`. Some of these columns are self-explanatory, while others may require some additional research to understand. For instance, you might ask what the difference between a borrower and a partner is, or you may need to search for other industry specific terms.

Looking at this dataframe, you might start to develop questions, for instance, considering how loan amount varies over time, or what sectors receive the most funding. Maybe you can look at how geographical location impacts loans given or received. It's good to make note of these ideas and revisit them in the context of the prompt - but more on that later!

When brainstorming these ideas, it's important to take into account what information your data does and doesn't include - missing or incomplete data is very common, and it may limit your analysis options. This brings us to our next topic: Data Cleaning.

## Data Cleaning

Before you start performing any analysis, you'll need to make sure the data is free of obvious errors and inconsistencies and formatted properly - a process known as data cleaning. What we've just imported is considered raw data, which may lack headers, contain nonsensical values, incomplete or duplicated data, inconsistent or inappropriate data types (e.g. numbers stored as strings), wrong category labels, and unknown or unexpected character encoding. These issues are caused by a variety of factors. For instance, data collected by surveys often includes inconsistent formatting or data type issues because users sometimes input information incorrectly. For more detailed information on data cleaning, check out this resource: [Data Cleaning with R](https://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf).

It's good practice to resolve these problems by modifying, replacing, or deleting corrupted or unusable data early on before conducting analysis, as they tend to cause more trouble the later they are addressed. However, it's important not to waste time in an event like this, and that means not cleaning data which we won't end up using. For that reason, now is a good time to make note of any problematic features in your data, but hold off on the actual cleaning until you have a more concrete plan.

As an example, let's return to the `kiva_loans` dataframe. If we scroll through it, we'll find that some columns seem to have a lot of NAs.

![](data/kiva_loans_na.png)

```{r}
apply(kiva_loan[, 1:10], 2, function(x) sum(is.na(x)))
```

This code displays the number of NAs in the first ten columns. Most of the columns aren't missing much - for example, the `borrower_name` column only has three NAs, and the `sector` column has zero. However, the `paid_amount` column has more than three thousand NAs. Columns like this don't have a lot of useful information, so you probably don't want to spend time cleaning these variables.

## Project Planning

### Rough Plan

At this point, you have considered the prompt and have a general idea of the data structure. Now, let's formulate a rough plan for our analysis.

First, let's revisit the prompt: *Help understand what motivates people to lend money to developing-nation entrepreneurs and what factors are associated with paying these loans.*

We can split the prompt into two main ideas: **money lending motivations** and **loan repayment factors**. You will probably want to create different models that focus on each of these themes. With that in mind, you can come up with a plan for what types of models you'll want to create and which models are feasible given the provided data.

In order to evaluate lender's motivations, we can imagine two possible strategies. First, we observe that there is a `loan_because` variable in the `lender` file. The data dictionary tells us that this field contains free-text quotes from lenders explaining their motivation for lending; in other words, it is ideal for the prompt. However, utilizing this variable will require that we clean and analyze complex text strings. If we have background in or are interested in learning text-mining, this might be an appealing approach. If not, we move on to our second strategy: the data contains a variety of borrower characteristics and a `status` variable which includes information on whether or not an application was funded. We can look for evidence of systematic differences between funded and non-funded loans to reveal lender preferences, and from these preferences, we may be able to infer lender priorities and motivations.

In order to evaluate the factors associated with loan repayment, we might again invoke the `status variable`. Because the dataset includes information on in-progress loans, it is considered *censored* data, and so the techniques of survival analysis - a statistical technique for analyzing the expected time until an event occurs - may be relevant. However, if our team is unfamiliar with survival analysis, we might opt for a simpler approach: restricting the dataset to only completed loans such that all loans can be classified as either defaulted or paid. Then, we can address the prompt by preparing descriptive statistics and data visualization for the two groups, or by fitting a binary-response model.

With a rough plan for the models we would like to create and present, we can start distributing tasks among the group members and get to work!

### Timeline

So now you know a bit more about getting started with data analysis. How do we put it all together? Here is a sample timeline over 48 hours of what the work distribution might look like for a team of four members.

### Friday

6:00 PM - 8:00 PM

-   setup (downloading data, setting up repo, etc.)
-   brainstorming session (all members)
-   basic data cleaning (1 team member)
-   domain specific research on data provider (rest of team members)

8:00 PM - 10:00 PM

-   prompt driven exploratory data analysis (all members)

### Saturday

9:00 AM - 10:00 AM

-   team check in, finalize topics to examine
-   finalize models to create
-   split into teams of two

10:00 AM - 2:00 PM

-   both teams work on preliminary model fitting and evaluation

2:00 PM - 6:00 PM

-   both teams work on advanced or refined model fitting and evaluation

6:00 PM - 6:30 PM

-   team check in

6:30 PM - 10:00 PM

-   finish creating graphics and prepare results for presentation (all members)

### Sunday

9:00 AM - 12:00 PM

-   finish presentation (all members)
-   run through presentation

12:00 PM - judging

## Key Takeaways

Leave enough time to put together the presentation. Prioritize tasks so that you will have something to present at the end. Be clear about objectives and make sure everyone knows what they are working on.

Check in periodically to make sure you are staying on schedule. Don't be afraid to change plans - if you find yourself ahead of schedule, you might be able to incorporate other angles into your analysis. On the other hand, you might find that you won't be able to complete all the tasks you originally envisioned. That's alright! The most important thing is understanding what a realistic deliverable looks like for you and your team.

Good luck and have fun!
