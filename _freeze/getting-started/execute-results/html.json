{
  "hash": "12a52728c49b717782b87eda4509f227",
  "result": {
    "markdown": "---\ntitle: \"Your First DataFest Event\"\nauthor: \"Angela Kan\"\ndate: '2022-07-01'\noutput: \n  html_document: \n    toc: true\n    toc_float: true\n---\n\n\n\n\n## Introduction\n\nSo you're at your first DataFest event. In 48 hours, you will need to have cleaned your dataset, analyzed it, and collected your findings into a digestible slide format. That can seem like a daunting task, especially if you haven't conducted data analysis before. But don't worry! Here, we break the process down into several approachable steps by walking through analysis on the 2012 DataFest dataset. By the end of this tutorial, you'll be fully equipped to dive into your DataFest journey.\n\n## Preparation Phase\n\n### Setting up R\n\nFirst, you're going to need to choose a language and software. Python and R are two of the most popular choices for Data Science. For this tutorial, we'll be working in R.\n\nTo download R, go to this [link](https://cloud.r-project.org/) and choose the correct download option.\n\nYou should also [download](https://www.rstudio.com/products/rstudio/download/) RStudio. Once you've finished the download and opened RStudio, your IDE will look something like this:\n\n![](data/start_page.png)\n\nOpen an R Markdown file by clicking on File -\\> New File -\\> R Markdown.\n\n![](data/rscript.png)\n\nThis will open a file that looks like this:\n\n![](data/rmarkdown.png)\n\nThis file will include some basic starter code examples, which you can delete and replace with your own.\n\nIf you'd like to see the rendered version, click the knit button. The knitted starter code looks like this:\n\n![](data/knitted.png)\n\nFor more in depth help with R and R Markdown, check out these resources:\n\n-   [R for Data Science textbook](https://r4ds.had.co.nz/)\n-   [R resource](https://cran.r-project.org/doc/contrib/Paradis-rdebuts_en.pdf)\n-   [R Markdown tutorial](https://www.rstudio.com/resources/webinars/getting-started-with-r-markdown/)\n\n### Collaborative Data Science\n\nSince you'll be working in a collaborative setting for DataFest, you'll want to find a workflow that accommodates your entire team's needs. One great tool for this is GitHub, a code hosting platform for version control and collaboration. GitHub allows you to create a code repository shared among different collaborators. Everyone is able to make changes to the project and access each others changes.\n\nThis link offers an in-depth walkthrough on GitHub if you'd like more information: [GitHub walkthrough](https://docs.github.com/en/get-started/quickstart/hello-world)\n\nFor a project like this, it's a good idea to have everyone write individual code files with their assigned tasks. This can help minimize merge conflicts, which can take a lot of time to troubleshoot when files have multiple authors. It's also good practice to comment your code so your teammates can understand it. Make sure to knit frequently to share your progress as well!\n\n## Getting Started\n\nNow that we're all set up, we can assess the prompt and data we've been given to work with.\n\n**The prompt:** *Help understand what motivates people to lend money to developing-nation entrepreneurs and what factors are associated with paying these loans.*\n\n**The dataset:** In this tutorial, we'll be using the dataset from the 2012 DataFest event, provided by Kiva. Kiva is a non-profit micro finance organization which connects online lenders to entrepreneurs across the globe. The aim is to expand financial access to under served communities and help them thrive with the help of peer-to-peer lending. The dataset involves 4 main files and 2 supplementary files, as well as a data dictionary.\n\nA data dictionary is a collection of names, definitions, and attributes about data elements that are being used or captured in a database, information system, or part of a research project. It describes the meanings and purposes of data elements within the context of a project, and provides guidance on interpretation, accepted meanings and representation.\n\nThis is what the Kiva data dictionary looks like:\n\n![](data/datadict.png)\n\nTaking into account the outlined goal and provided data, you'll want to ask yourself what you intend to present by the end of your 48 hours. What questions do you want to answer? What visualizations will be appropriate?\n\nAt this step, we should browse the data to gain an understanding of what resources we have at our disposal and what analysis angles might make sense.\n\nFirst let's import some libraries. These libraries allow you to access a broad range of data analysis tools not included in the base R package.\n\n\\*Make sure to call `install.packages(\"package_name\")` in the console if you are getting package issues!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"tidyverse\")\n```\n:::\n\n\nNow, let's import the data. Make sure you've downloaded the data into the proper directory, then import it using the appropriate file path.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkiva_ll <- read_csv(\"data/lender_loans.csv\")\nkiva_lender <- read_csv(\"data/lenders.csv\")\nkiva_loan <- read_csv(\"data/loans.csv\")\nkiva_partner <- read_csv(\"data/partners.csv\")\n```\n:::\n\n\nWe can use the `head()` command to take a look at the downloaded data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(kiva_loan)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 75\n  loan_id borrower_name        borrower_image_… status funded_amount paid_amount\n    <dbl> <chr>                           <dbl> <chr>          <dbl>       <dbl>\n1      89 Rose                              246 paid             500         500\n2     102 Simon                             264 paid             500         500\n3     122 Joseph                            287 paid             500         500\n4     124 Richard                           294 paid             500         500\n5     147 Anonymous                      726677 refun…             0          NA\n6     193 Tesla Rudibel Andino              344 paid             375         375\n# … with 69 more variables: activity <chr>, sector <chr>, use <chr>,\n#   location.country_code <chr>, location.country <chr>, location.town <chr>,\n#   location.geo.level <chr>, lat <dbl>, lon <dbl>, partner_id <dbl>,\n#   posted_yr <dbl>, posted_mo <dbl>, posted_day <dbl>, posted_hr <dbl>,\n#   posted_min <dbl>, posted_sec <dbl>, loan_amount <dbl>,\n#   terms.disbursal_amount <dbl>, terms.loan_amount <dbl>,\n#   terms.loss_liability.nonpayment <chr>, …\n```\n:::\n:::\n\n\nFor a clearer view of the entire dataframe, we can also open it up in Excel or any csv file viewer:\n\n![](data/kiva_loans.png)\n\nWhat relationships do you notice from the dataframe?\n\nAt a glance, we can see columns like status, activity, and sector. Some of these columns are self-explanatory. Others may require some additional googling to understand. For instance, you might ask what the difference between a borrower and a partner is, or you may need to google other industry specific terms.\n\nLooking at this dataframe, you could examine how loans are given out over time, or what sectors receive the most funding. Maybe you can look at how geographical location impacts loans given or received.\n\nWhen brainstorming these ideas, it's important to take into account what information your data does and doesn't include - missing or incomplete data is very common, and it may limit your analysis options. This brings us to our next topic: Data Cleaning.\n\n## Data Cleaning\n\nBefore you can start performing analysis, you'll need to make sure the data is clean and workable. What we've just imported is considered raw data, which may lack headers, contain wrong data types (e.g. numbers stored as strings), wrong category labels, unknown or unexpected character encoding and so on. These issues are caused by a variety of factors. For instance, data collected by surveys often includes inconsistent formatting or data type issues because users sometimes input information incorrectly. Keep an eye out for common problems such as duplicates, missing fields, and inconsistencies.\n\nIt's good practice to resolve these problems early on before conducting analysis, as they tend to cause more trouble the later they are addressed.\n\nAs an example, let's return to the `kiva_loans` dataframe. If we scroll through it, we'll find that some columns seem to have a lot of NAs.\n\n![](data/kiva_loans_na.png)\n\nLet's pick some columns and check how many NAs there are exactly:\n\n\n::: {.cell}\n\n```{.r .cell-code}\napply(kiva_loan[\"planned_expiration_yr\"], 2, function(x) sum(is.na(x)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nplanned_expiration_yr \n                89856 \n```\n:::\n\n```{.r .cell-code}\napply(kiva_loan[\"sector\"], 2, function(x) sum(is.na(x)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nsector \n     0 \n```\n:::\n:::\n\n\nIt looks like some columns have a lot of NAs. These columns aren't going to help very much and can be dropped.\n\nThat's one example of data cleanliness. However, it's important not to waste time in an event like this, and that means not cleaning data which we won't end up using. For that reason, now is a good time to make note of any problematic features in your data, but hold off on the actual cleaning until you have a more concrete plan\n\nCheck out this resource for more detailed information on data cleaning: [Data Cleaning with R](https://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf)\n\n## Project Planning\n\n### Rough Plan\n\nSo now you've taken a look at the prompt and data and have a general idea of what angle you would like to take for your analysis. Let's start by coming up with a rough plan.\n\nFirst, let's revisit the prompt: *Help understand what motivates people to lend money to developing-nation entrepreneurs and what factors are associated with paying these loans.*\n\nWe can split the prompt into two main ideas: **money lending motivations** and **loan repayment factors**. You will probably want to create different models that focus on each of these themes. With that in mind, you can come up with a plan for what types of models you'll want to create and which models are feasible given the provided data.\n\nAs an example, the `kiva_loans` dataframe includes information on loan amounts and years. We can use this to see which years have the highest loan amounts as well as how loan amounts change over time. We also have geographical information for the loans - we could create a model to see where most loans are coming from or where they are given to. Take some time to think about the relationships you can observe and ask yourself which ones are the most relevant for the questions you want to answer.\n\nOnce you've determined the models you would like to create and present, you can start distributing tasks among your group and get to work!\n\n### Timeline\n\nSo now you know a bit more about getting started with data analysis. How do we put it all together? Here is a sample timeline over 48 hours of what the work distribution might look like for a team of four members.\n\n### Friday\n\n-   6:00 PM - kickoff!\n\n-   7:00 PM - brainstorming session (all members)\n\n-   8:00 PM - basic data cleaning (1 team member) - formatting variables - making sure missing conventions are consistent\n\n-   10:00 PM - exploratory data analysis (all members)\n\n### Saturday\n\n-   9:00 AM - team check in, finalize topics to examine - finalize models to create\n\n-   10:00 AM - split into teams of two - team 1 on model 1 - team 2 on model 2\n\n-   12:00 PM - lunch break\n\n-   12:30 PM - team 1 on model 1 - team 2 on model 2\n\n-   5:00 PM - dinner break\n\n-   6:00 PM - team check in\n\n-   7:00 PM - team 1 on model 1 - team 2 on model 2\n\n-   10:00 PM - start working on presentation (all members)\n\n### Sunday\n\n-   9:00 AM - finish presentation (all members) - run through presentation\n\n-   12:00 PM - judging\n\nThe important takeaways:\n\nLeave enough time to put together the presentation. Prioritize tasks so that you will have something to present at the end. Be clear about objectives and make sure everyone knows what they are working on.\n\nCheck in periodically to make sure you are staying on schedule. Don't be afraid to change plans - if you find yourself ahead of schedule, you might be able to incorporate other angles into your analysis. On the other hand, you might find that you won't be able to complete all the tasks you originally envisioned. That's alright! The most important thing is understanding what a realistic deliverable looks like for you and your team.\n\nGood luck and have fun!\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}